<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读 | Relativity suis's Blog</title><meta name="author" content="Relativity suis"><meta name="copyright" content="Relativity suis"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;542465517 Abstract The capacity of a neural">
<meta property="og:type" content="article">
<meta property="og:title" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">
<meta property="og:url" content="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html">
<meta property="og:site_name" content="Relativity suis&#39;s Blog">
<meta property="og:description" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;542465517 Abstract The capacity of a neural">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/486.jpg">
<meta property="article:published_time" content="2025-01-07T09:40:30.000Z">
<meta property="article:modified_time" content="2025-01-07T16:58:08.288Z">
<meta property="article:author" content="Relativity suis">
<meta property="article:tag" content="MoE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/486.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/486.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/suis1.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Relativity suis's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-07T16:58:08.288Z" title="更新于 2025-01-08 00:58:08">2025-01-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-论文研读"><a href="#Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-论文研读" class="headerlink" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读"></a>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a></p>
<p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542465517">https://zhuanlan.zhihu.com/p/542465517</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increas-ing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We in-troduce a Sparsely-Gated Mixture-of-Experts layer(MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</p>
</blockquote>
<p>神经网络吸收信息的能力受限于其参数数量。条件计算（conditional computation）是一种理论上提出的方法，通过在每个样本基础上激活网络的一部分，从而在不显著增加计算量的情况下大幅提升模型容量。然而，在实践中，这一方法面临显著的算法和性能挑战。在本研究中，我们解决了这些挑战，最终实现了条件计算的潜力，在现代GPU集群上实现了超过1000倍的模型容量提升，同时仅带来轻微的计算效率损失。我们引入了一种<strong>稀疏门控的专家混合层</strong>（Sparsely-Gated Mixture-of-Experts, MoE），该层包含多达数千个前馈子网络。一个可训练的门控网络为每个样本确定这些专家的稀疏组合。我们将MoE应用于语言建模和机器翻译任务，在这些任务中，模型容量对于吸收训练语料库中大量知识至关重要。我们提出了一种模型架构，其中包含多达1370亿参数的MoE被卷积地应用于堆叠的LSTM层之间。在大型语言建模和机器翻译基准测试中，这些模型以较低的计算成本取得了显著优于当前最先进技术的结果。</p>
<hr>
<p>文章声称首次解决了先前条件计算面临的所有挑战，仅以微小的计算效率损失换取了超过1000倍的模型容量提升，并显著提高了公共语言建模和翻译数据集上的SOTA。</p>
<h2 id="The-Approach：THE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER"><a href="#The-Approach：THE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER" class="headerlink" title="The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER"></a>The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</h2><p>作者实现条件计算的方法是 引入一种新型的通用神经网络组件：<strong>稀疏门控专家混合层</strong>（Sparsely-Gated Mixture-of-Experts Layer, MoE）。MoE 由若干专家组成，每个专家是一个简单的前馈神经网络，同时包含一个可训练的门控网络，用于为每个输入选择专家的稀疏组合（见下图），网络的所有部分通过反向传播联合训练。</p>
<p>尽管所引入的技术是通用的，但在本文中，作者主要关注了语言建模和机器翻译任务，这些任务已知能够从超大规模模型中受益。具体而言，我们在堆叠的 LSTM 层（Hochreiter &amp; Schmidhuber, 1997）之间卷积地应用 MoE，如下图所示。MoE 在<strong>文本的每个位置</strong>被调用一次，每个位置可能选择不同的专家组合，不同的专家往往会基于句法和语义高度专业化。另外，本文作者的工作建立在将 MoEs 作为通用神经网络组件的基础上</p>
<p><img src="/images/moe17-1.png" alt="moe17-1"><br>图 1</p>
<h2 id="The-Structure-of-the-MoE-Layer"><a href="#The-Structure-of-the-MoE-Layer" class="headerlink" title="The Structure of the MoE Layer"></a>The Structure of the MoE Layer</h2><p>混合专家（MoE）层由一组 $n$ 个”专家网络” $E_1,\cdots,E_n$ 和一个输出为稀疏 $n$ 维向量的”门控网络” $G$ 组成。图 1 （上图）展示了MoE模块的概览。专家本身是神经网络，每个专家都有其自己的参数。虽然原则上我们只要求专家接受相同大小的输入并产生相同大小的输出，但在本文的初步研究中，我们将自己限制在模型是具有相同架构但参数不同（separate parameters）的前馈网络的情况。</p>
<p>让我们用 $G(x)$ 和 $E_i(x)$ 分别表示给定输入 $x$ 时门控网络的输出和第 $i$ 个专家网络的输出。MoE模块的输出 $y$ 可以写作：</p>
<script type="math/tex; mode=display">
y = \sum_{i=1}^n G(x)_iE_i(x) \tag{1}</script><p>基于 $G(x)$ 输出的<strong>稀疏性</strong>，我们可以节省计算量。当 $G(x)_i = 0$ 时，我们不需要计算 $E_i(x)$。在我们的实验中，<strong>我们有多达数千个专家，但对每个样例只需要评估其中少数几个</strong>。</p>
<p>如果专家数量非常大，我们可以通过使用<strong>两级层次化 MoE</strong>（a two-level hierarchical MoE）来减少分支因子。在一个层次化 MoE 中，主门控网络选择 “专家” 的<strong>稀疏加权组合</strong>，每个专家本身都是具有自己门控网络的次级混合专家。在下文中主要关注普通的 MoE，作者在论文的附录B中提供了关于层次化 MoE 的更多细节。</p>
<p>我们的实现与其他条件计算模型相关，具有简单权重矩阵作为专家的 MoE 类似于(Cho &amp; Bengio, 2014)中提出的参数化权重矩阵。具有一个隐藏层的专家的 MoE 类似于(Bengio et al., 2015)中描述的分块式 dropout，其中 dropout 层被夹在完全激活的层之间。</p>
<h3 id="Gating-Network"><a href="#Gating-Network" class="headerlink" title="Gating Network"></a>Gating Network</h3><ol>
<li><strong>Softmax Gating</strong>：</li>
</ol>
<p>一个简单的非稀疏的门控函数是，将输入与一个可训练的权重矩阵相乘，然后对其应用 Softmax：</p>
<script type="math/tex; mode=display">
G_{\sigma} = Softmax(x \cdot W_g) \tag{2}</script><ol>
<li><strong>Noisy Top-K Gating</strong>：</li>
</ol>
<p>我们在Softmax门控网络中增加两个组件（components）：稀疏性和噪声（sparsity and noise），在应用 softmax 函数之前，我们添加可调高斯噪声（tunable Gaussian noise），然后只保留前k个值，并将其余值设置为负无穷（这会导致相应的门控值等于零），其稀疏性有助于节省计算。虽然这种形式的稀疏性会在门控函数输出中产生一些理论上令人担忧的不连续点，但在实践中尚未观察到这是一个问题。每个组件中的噪声数量由第二个可训练的权重矩阵 $W_{noise}$ 控制。</p>
<script type="math/tex; mode=display">
G(x) = Softmax(KeepTopK(H(x),~k)) \tag{3}</script><script type="math/tex; mode=display">
H(x)_i = (x \cdot W_{g})_i + StandardNormal() \cdot Softplus((x \cdot W_{noise})_i) \tag{4}</script><script type="math/tex; mode=display">
KeepTopK(v,~k)_i = 
\begin{cases}
v_i & \text{if }v_i \text{ is in the top } k \text{ elements of } v . \\
-\infty & \text{otherwise.}
\end{cases}
\tag{5}</script><h2 id="BALANCING-EXPERT-UTILIZATION"><a href="#BALANCING-EXPERT-UTILIZATION" class="headerlink" title="BALANCING EXPERT UTILIZATION"></a>BALANCING EXPERT UTILIZATION</h2><p>作者在实验中发现，门控网络倾向于收敛到一种状态：其总是为少数几个专家分配大的权重。这种不平衡是自我强化的（self-reinforcing），因为受青睐的专家会训练地更快，因而也被门控网络选择地更多。</p>
<p>作者采用了一种软约束方法，其将专家相对于一批训练样本的重要性定义为该专家在这批样本上门控值的批次总和，并定义了一个额外的损失项 $L<em>{\text{importance}}$，并将其添加到模型的总体损失函数中。该损失等于重要性值集合的变异系数的平方，再乘以一个手动调整的<strong>缩放因子</strong> $w</em>{\text{importance}}$。这一额外的损失项鼓励所有专家具有同等的重要性。</p>
<script type="math/tex; mode=display">
Importance(X) = \sum_{x \in X} G(x) \tag{6}</script><script type="math/tex; mode=display">
L_{importance}(X) = w_{importance} \cdot CV(Importance(X))^2 \tag{7}</script><hr>
<p>与1991年的 Adaptive-Mixtures-of-Local-Experts 中（具体可见：）做的工作对比：</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Relativity suis</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Relativity suis's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MoE/">MoE</a></div><div class="post-share"><div class="social-share" data-image="/img/486.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Adaptive Mixtures of Local Experts 论文研读</div></div><div class="info-2"><div class="info-item-1">Adaptive Mixtures of Local Experts 论文研读论文链接：https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf 参考链接：https://zhuanlan.zhihu.com/p/423447025 Abstract We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-07</div><div class="info-item-2">Adaptive Mixtures of Local Experts 论文研读</div></div><div class="info-2"><div class="info-item-1">Adaptive Mixtures of Local Experts 论文研读论文链接：https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf 参考链接：https://zhuanlan.zhihu.com/p/423447025 Abstract We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/486.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Relativity suis</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Gcy-shili"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Blog 积极更新中！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB"><span class="toc-number">1.</span> <span class="toc-text">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Approach%EF%BC%9ATHE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER"><span class="toc-number">1.2.</span> <span class="toc-text">The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Structure-of-the-MoE-Layer"><span class="toc-number">1.3.</span> <span class="toc-text">The Structure of the MoE Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gating-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">Gating Network</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BALANCING-EXPERT-UTILIZATION"><span class="toc-number">1.4.</span> <span class="toc-text">BALANCING EXPERT UTILIZATION</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</a><time datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读">Adaptive Mixtures of Local Experts 论文研读</a><time datetime="2025-01-06T16:29:09.000Z" title="发表于 2025-01-07 00:29:09">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/RoPE/" title="RoPE">RoPE</a><time datetime="2025-01-05T16:16:07.000Z" title="发表于 2025-01-06 00:16:07">2025-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/05/RMSNorm/" title="RMSNorm">RMSNorm</a><time datetime="2025-01-05T15:58:03.000Z" title="发表于 2025-01-05 23:58:03">2025-01-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/04/Attention%20Pt.1.%20Understanding-from-seq2seq-to-attention/" title="Understanding from seq2seq to attention">Understanding from seq2seq to attention</a><time datetime="2025-01-04T04:54:59.000Z" title="发表于 2025-01-04 12:54:59">2025-01-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Relativity suis</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">"Suis is all you need"</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>