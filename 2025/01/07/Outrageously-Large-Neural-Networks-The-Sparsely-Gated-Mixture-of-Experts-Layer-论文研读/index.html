<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读 | Relativity suis's Blog</title><meta name="author" content="Relativity suis"><meta name="copyright" content="Relativity suis"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;542465517 Abstract The capacity of a neural">
<meta property="og:type" content="article">
<meta property="og:title" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">
<meta property="og:url" content="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html">
<meta property="og:site_name" content="Relativity suis&#39;s Blog">
<meta property="og:description" content="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1701.06538 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;542465517 Abstract The capacity of a neural">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/486.jpg">
<meta property="article:published_time" content="2025-01-07T09:40:30.000Z">
<meta property="article:modified_time" content="2025-01-07T17:24:59.072Z">
<meta property="article:author" content="Relativity suis">
<meta property="article:tag" content="MoE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/486.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/486.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/suis1.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Relativity suis's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-07T17:24:59.072Z" title="更新于 2025-01-08 01:24:59">2025-01-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-论文研读"><a href="#Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-论文研读" class="headerlink" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读"></a>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a></p>
<p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542465517">https://zhuanlan.zhihu.com/p/542465517</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increas-ing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We in-troduce a Sparsely-Gated Mixture-of-Experts layer(MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.</p>
</blockquote>
<p>神经网络吸收信息的能力受限于其参数数量。条件计算（conditional computation）是一种理论上提出的方法，通过在每个样本基础上激活网络的一部分，从而在不显著增加计算量的情况下大幅提升模型容量。然而，在实践中，这一方法面临显著的算法和性能挑战。在本研究中，我们解决了这些挑战，最终实现了条件计算的潜力，在现代GPU集群上实现了超过1000倍的模型容量提升，同时仅带来轻微的计算效率损失。我们引入了一种<strong>稀疏门控的专家混合层</strong>（Sparsely-Gated Mixture-of-Experts, MoE），该层包含多达数千个前馈子网络。一个可训练的门控网络为每个样本确定这些专家的稀疏组合。我们将MoE应用于语言建模和机器翻译任务，在这些任务中，模型容量对于吸收训练语料库中大量知识至关重要。我们提出了一种模型架构，其中包含多达1370亿参数的MoE被卷积地应用于堆叠的LSTM层之间。在大型语言建模和机器翻译基准测试中，这些模型以较低的计算成本取得了显著优于当前最先进技术的结果。</p>
<hr>
<p>文章声称首次解决了先前条件计算面临的所有挑战，仅以微小的计算效率损失换取了超过1000倍的模型容量提升，并显著提高了公共语言建模和翻译数据集上的SOTA。</p>
<h2 id="The-Approach：THE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER"><a href="#The-Approach：THE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER" class="headerlink" title="The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER"></a>The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</h2><p>作者实现条件计算的方法是 引入一种新型的通用神经网络组件：<strong>稀疏门控专家混合层</strong>（Sparsely-Gated Mixture-of-Experts Layer, MoE）。MoE 由若干专家组成，每个专家是一个简单的前馈神经网络，同时包含一个可训练的门控网络，用于为每个输入选择专家的稀疏组合（见下图），网络的所有部分通过反向传播联合训练。</p>
<p>尽管所引入的技术是通用的，但在本文中，作者主要关注了语言建模和机器翻译任务，这些任务已知能够从超大规模模型中受益。具体而言，我们在堆叠的 LSTM 层（Hochreiter &amp; Schmidhuber, 1997）之间卷积地应用 MoE，如下图所示。MoE 在<strong>文本的每个位置</strong>被调用一次，每个位置可能选择不同的专家组合，不同的专家往往会基于句法和语义高度专业化。另外，本文作者的工作建立在将 MoEs 作为通用神经网络组件的基础上</p>
<blockquote>
<p><img src="/images/moe17-1.png" alt="moe17-1"><br>图1</p>
</blockquote>
<h2 id="The-Structure-of-the-MoE-Layer"><a href="#The-Structure-of-the-MoE-Layer" class="headerlink" title="The Structure of the MoE Layer"></a>The Structure of the MoE Layer</h2><p>混合专家（MoE）层由一组 $n$ 个”专家网络” $E_1,\cdots,E_n$ 和一个输出为稀疏 $n$ 维向量的”门控网络” $G$ 组成。图 1 （上图）展示了MoE模块的概览。专家本身是神经网络，每个专家都有其自己的参数。虽然原则上我们只要求专家接受相同大小的输入并产生相同大小的输出，但在本文的初步研究中，我们将自己限制在模型是具有相同架构但参数不同（separate parameters）的前馈网络的情况。</p>
<p>让我们用 $G(x)$ 和 $E_i(x)$ 分别表示给定输入 $x$ 时门控网络的输出和第 $i$ 个专家网络的输出。MoE模块的输出 $y$ 可以写作：</p>
<script type="math/tex; mode=display">
y = \sum_{i=1}^n G(x)_iE_i(x) \tag{1}</script><p>基于 $G(x)$ 输出的<strong>稀疏性</strong>，我们可以节省计算量。当 $G(x)_i = 0$ 时，我们不需要计算 $E_i(x)$。在我们的实验中，<strong>我们有多达数千个专家，但对每个样例只需要评估其中少数几个</strong>。</p>
<p>如果专家数量非常大，我们可以通过使用<strong>两级层次化 MoE</strong>（a two-level hierarchical MoE）来减少分支因子。在一个层次化 MoE 中，主门控网络选择 “专家” 的<strong>稀疏加权组合</strong>，每个专家本身都是具有自己门控网络的次级混合专家。在下文中主要关注普通的 MoE，作者在论文的附录B中提供了关于层次化 MoE 的更多细节。</p>
<p>我们的实现与其他条件计算模型相关，具有简单权重矩阵作为专家的 MoE 类似于(Cho &amp; Bengio, 2014)中提出的参数化权重矩阵。具有一个隐藏层的专家的 MoE 类似于(Bengio et al., 2015)中描述的分块式 dropout，其中 dropout 层被夹在完全激活的层之间。</p>
<h3 id="Gating-Network"><a href="#Gating-Network" class="headerlink" title="Gating Network"></a>Gating Network</h3><ol>
<li><strong>Softmax Gating</strong>：</li>
</ol>
<p>一个简单的非稀疏的门控函数是，将输入与一个可训练的权重矩阵相乘，然后对其应用 Softmax：</p>
<script type="math/tex; mode=display">
G_{\sigma} = Softmax(x \cdot W_g) \tag{2}</script><ol>
<li><strong>Noisy Top-K Gating</strong>：</li>
</ol>
<p>我们在Softmax门控网络中增加两个组件（components）：稀疏性和噪声（sparsity and noise），在应用 softmax 函数之前，我们添加可调高斯噪声（tunable Gaussian noise），然后只保留前k个值，并将其余值设置为负无穷（这会导致相应的门控值等于零），其稀疏性有助于节省计算。虽然这种形式的稀疏性会在门控函数输出中产生一些理论上令人担忧的不连续点，但在实践中尚未观察到这是一个问题。每个组件中的噪声数量由第二个可训练的权重矩阵 $W_{noise}$ 控制。</p>
<script type="math/tex; mode=display">
G(x) = Softmax(KeepTopK(H(x),~k)) \tag{3}</script><script type="math/tex; mode=display">
H(x)_i = (x \cdot W_{g})_i + StandardNormal() \cdot Softplus((x \cdot W_{noise})_i) \tag{4}</script><script type="math/tex; mode=display">
KeepTopK(v,~k)_i = 
\begin{cases}
v_i & \text{if }v_i \text{ is in the top } k \text{ elements of } v . \\
-\infty & \text{otherwise.}
\end{cases}
\tag{5}</script><h2 id="BALANCING-EXPERT-UTILIZATION"><a href="#BALANCING-EXPERT-UTILIZATION" class="headerlink" title="BALANCING EXPERT UTILIZATION"></a>BALANCING EXPERT UTILIZATION</h2><p>作者在实验中发现，门控网络倾向于收敛到一种状态：其总是为少数几个专家分配大的权重。这种不平衡是自我强化的（self-reinforcing），因为受青睐的专家会训练地更快，因而也被门控网络选择地更多。</p>
<p>作者采用了一种软约束方法，其将专家相对于一批训练样本的重要性定义为该专家在这批样本上门控值的批次总和，并定义了一个额外的损失项 $L<em>{\text{importance}}$，并将其添加到模型的总体损失函数中。该损失等于重要性值集合的变异系数的平方，再乘以一个手动调整的<strong>缩放因子</strong> $w</em>{\text{importance}}$。这一额外的损失项鼓励所有专家具有同等的重要性。</p>
<script type="math/tex; mode=display">
Importance(X) = \sum_{x \in X} G(x) \tag{6}</script><script type="math/tex; mode=display">
L_{importance}(X) = w_{importance} \cdot CV(Importance(X))^2 \tag{7}</script><hr>
<p>与1991年的 Adaptive-Mixtures-of-Local-Experts 中（具体可见：<a target="_blank" rel="noopener" href="https://gcy-shili.github.io/2025/01/07/Adaptive-Mixtures-of-Local-Experts-论文研读/">Adaptive Mixtures of Local Experts 论文研读 | Relativity suis’s Blog</a>）做的工作对比：这里的 MoE 主要有两个区别：</p>
<ol>
<li>稀疏门控：不是所有专家都会起作用，而是极少数的专家会被使用来进行推理，这种稀疏性，也使得我们可以使用海量的专家来把模型容量做的超级大。</li>
<li>token-level：前者的工作，是 sample-level 的，即不同的样本，使用不同的专家，但是这篇则是 token-level 的，一个句子中不同的 token 使用不同的专家，如论文中说：</li>
</ol>
<blockquote>
<p> The MoE is called once for <strong>each position</strong> in the text, selecting a potentially different combination of experts at each position.</p>
</blockquote>
<p>这篇文章的工作是在 RNN 中添加了 MoE 层，如上图（图1）所示，即每个 token 对应的位置（position）都会有一个 MoE 层，每个 MoE 层包含了一堆的专家（Experts,  $\text{Expert}_{1 \cdots n}$ ），每个专家都是一个小型的 FFN，Gating Network 则会根据当前 position 的输入，选择少数几个专家来进行计算。</p>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><blockquote>
<p>问题1：如何控制门控网络输出的为一个稀疏的权重向量，具体来说Noisy Top-K门控是如何实现这一点的？<br>问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？<br>问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？</p>
</blockquote>
<h3 id="问题1：如何控制门控网络输出为一个稀疏的权重向量，具体来说-Noisy-Top-K-门控是如何实现这一点的？"><a href="#问题1：如何控制门控网络输出为一个稀疏的权重向量，具体来说-Noisy-Top-K-门控是如何实现这一点的？" class="headerlink" title="问题1：如何控制门控网络输出为一个稀疏的权重向量，具体来说 Noisy Top-K 门控是如何实现这一点的？"></a><strong>问题1：如何控制门控网络输出为一个稀疏的权重向量，具体来说 Noisy Top-K 门控是如何实现这一点的？</strong></h3><h4 id="理解稀疏权重向量"><a href="#理解稀疏权重向量" class="headerlink" title="理解稀疏权重向量"></a><strong>理解稀疏权重向量</strong></h4><p>在混合专家模型（Mixture-of-Experts, MoE）中，门控网络的任务是为每个输入选择一部分专家进行计算。<strong>稀疏权重向量</strong>意味着在所有专家中，只有少数几个（如前K个）被激活并用于当前输入，而其余的专家权重为零，从而节省计算资源。</p>
<h4 id="Noisy-Top-K-门控的实现步骤"><a href="#Noisy-Top-K-门控的实现步骤" class="headerlink" title="Noisy Top-K 门控的实现步骤"></a><strong>Noisy Top-K 门控的实现步骤</strong></h4><p>Noisy Top-K门控机制通过以下步骤实现稀疏权重向量：</p>
<ol>
<li><p><strong>计算初始门控得分（Gating Scores）：</strong></p>
<p>对于给定输入 $x$，门控网络首先计算每个专家的初始得分：</p>
<script type="math/tex; mode=display">
S_i = x \cdot W_g</script><p>其中，$W_g$ 是门控网络的权重矩阵，$S_i$ 是第 $i$ 个专家的得分。</p>
</li>
<li><p><strong>添加可调的高斯噪声（Add Tunable Gaussian Noise）：</strong></p>
<p>为了增加选择的多样性和鲁棒性，向每个专家的得分中添加可调的高斯噪声：</p>
<script type="math/tex; mode=display">
H_i = S_i + \text{StandardNormal()} \times \text{Softplus}(x \cdot W_{noise})</script><p>其中：</p>
<ul>
<li>$\text{StandardNormal()}$ 表示从标准正态分布（均值为0，方差为1）中采样的随机噪声。</li>
<li>$W_{noise}$ 是另一个可训练的权重矩阵，用于控制噪声的幅度。</li>
<li>$\text{Softplus}(\cdot)$ 是一种平滑的激活函数，确保噪声幅度为正。</li>
</ul>
</li>
<li><p><strong>选择Top-K得分（Keep Top-K）：</strong></p>
<p>从加噪后的得分 $H$ 中选择前K个最高的值，其余的设置为负无穷：</p>
<script type="math/tex; mode=display">
\text{KeepTopK}(H, K)_i =
\begin{cases}
H_i & \text{如果 } H_i \text{ 是前 } K \text{ 个最大值之一} \\
-\infty & \text{否则}
\end{cases}</script><p>这样，只有前K个专家的得分保持有效，其他专家的得分变为负无穷，经过Softmax后对应的权重为零。</p>
</li>
<li><p><strong>应用Softmax函数（Apply Softmax）：</strong></p>
<p>对保留后的得分应用 Softmax 函数，得到稀疏的权重向量：</p>
<script type="math/tex; mode=display">
G(x)_i = \text{Softmax}(\text{KeepTopK}(H, K)_i)</script><p>由于大多数值为负无穷，Softmax 会将这些值对应的权重计算为零，仅前 K 个专家拥有非零权重。</p>
<h3 id="问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？"><a href="#问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？" class="headerlink" title="问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？"></a><strong>问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？</strong></h3><h4 id="可调的高斯噪声的定义"><a href="#可调的高斯噪声的定义" class="headerlink" title="可调的高斯噪声的定义"></a><strong>可调的高斯噪声的定义</strong></h4></li>
</ol>
<p><strong>可调的高斯噪声</strong>是指具有可调参数（如均值和方差）的高斯（正态）分布噪声。在 Noisy Top-K 门控中，这种噪声被添加到门控得分中，以实现更灵活和鲁棒的专家选择。<br>具体来说，论文中使用的噪声项定义为：</p>
<script type="math/tex; mode=display">
\text{Noise}_i = \text{StandardNormal()} \times \text{Softplus}(x \cdot W_{noise})</script><p>其中：</p>
<ul>
<li>$\text{StandardNormal()}$ 是从标准正态分布（均值为0，方差为1）中采样的随机噪声。</li>
<li>$W_{noise}$ 是一个可训练的权重矩阵，用于控制噪声的幅度。</li>
<li>$\text{Softplus}(\cdot)$ 确保噪声幅度为正。<h4 id="为什么添加高斯噪声可以增加模型的鲁棒性？"><a href="#为什么添加高斯噪声可以增加模型的鲁棒性？" class="headerlink" title="为什么添加高斯噪声可以增加模型的鲁棒性？"></a><strong>为什么添加高斯噪声可以增加模型的鲁棒性？</strong></h4><strong>增加模型鲁棒性</strong>的原因主要包括以下几个方面：</li>
</ul>
<ol>
<li><p><strong>促进专家的多样性（Diversity of Experts）：</strong></p>
<p>添加噪声打破了门控网络对专家选择的确定性，使得在不同训练迭代或不同输入下，专家的选择更加多样化。这有助于避免模型过度依赖某些特定的专家，从而使得各个专家能够学习到更多不同的特征和表示。</p>
</li>
<li><p><strong>防止过拟合（Preventing Overfitting）：</strong></p>
<p>如果门控网络总是选择同样的专家，某些专家可能会过度训练，而其他专家则几乎不被训练。噪声的引入鼓励门控网络探索不同的专家组合，避免了特定专家的过拟合。</p>
</li>
<li><p><strong>提高模型的泛化能力（Improving Generalization）：</strong></p>
<p>通过引入噪声，模型在面对未见过的数据时，能够更好地适应不同的专家组合，提升了整体的泛化能力。这意味着模型在处理新样本时，能够更灵活地调用不同的专家，从而更准确地进行预测或翻译。</p>
</li>
<li><p><strong>增强训练的稳定性（Enhancing Training Stability）：</strong></p>
<p>噪声的引入可以平滑门控网络的决策边界，使得模型在训练过程中更不容易陷入局部最优解。这样，模型能够更全面地探索专家空间，找到更优的参数配置。</p>
<h4 id="具体机制解释"><a href="#具体机制解释" class="headerlink" title="具体机制解释"></a><strong>具体机制解释</strong></h4><p>在 Noisy Top-K 门控中，噪声的引入是有目的的：</p>
</li>
</ol>
<ul>
<li><p><strong>探索性选择（Exploratory Selection）：</strong></p>
<p>噪声打破了门控网络对专家得分的严格排序，允许一些得分较低但仍具潜力的专家被选中。这种探索性选择有助于发现更多有用的专家，提高整体模型的表现。</p>
</li>
<li><p><strong>平滑专家的利用（Smoothing Expert Utilization）：</strong></p>
<p>通过引入噪声，门控网络不会总是选择同样的专家，这有助于平衡各个专家的使用频率，避免某些专家被频繁使用而其他专家被忽略。</p>
<h3 id="问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？"><a href="#问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？" class="headerlink" title="问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？"></a><strong>问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？</strong></h3><h4 id="专家利用不平衡的问题"><a href="#专家利用不平衡的问题" class="headerlink" title="专家利用不平衡的问题"></a><strong>专家利用不平衡的问题</strong></h4></li>
</ul>
<p>在 MoE 模型中，由于门控网络的决策，某些专家可能会被频繁选择，而其他专家则很少或根本不被使用。这种<strong>专家利用不平衡</strong>会导致：</p>
<ol>
<li><strong>有限的模型容量</strong>：尽管模型总体参数量大，但实际只有少数专家在工作，限制了模型的表达能力。</li>
<li><strong>训练不充分</strong>：被频繁选择的专家会得到更多的训练，而未被选择的专家几乎不被训练，导致其性能不足。</li>
<li><strong>资源浪费</strong>：部分专家被闲置，浪费了模型的潜在资源和计算能力。</li>
</ol>
<h4 id="引入额外损失项的目的"><a href="#引入额外损失项的目的" class="headerlink" title="引入额外损失项的目的"></a><strong>引入额外损失项的目的</strong></h4><p>为了 <strong>平衡专家的利用率</strong>，论文提出在损失函数中引入一个额外的损失项 $L_{\text{importance}}$。这个损失项旨在鼓励所有专家被均衡地使用，从而解决专家利用不平衡的问题。</p>
<h4 id="具体实现步骤"><a href="#具体实现步骤" class="headerlink" title="具体实现步骤"></a><strong>具体实现步骤</strong></h4><ol>
<li><p><strong>定义专家的重要性（Importance）：</strong></p>
<p>对于一个批次的训练样本 $X$，定义每个专家的重要性为<strong>该专家在这批样本中被选择的总和</strong>：</p>
<script type="math/tex; mode=display">
\text{Importance}(X) = \sum_{x \in X} G(x)</script><p>其中，$G(x)$ 是输入 $x$ 的门控权重向量，表示各专家的选择权重。</p>
</li>
<li><p><strong>计算变异系数（Coefficient of Variation, CV）：</strong></p>
<p>变异系数是标准差与均值的比值，用于衡量数据的相对变异程度：</p>
<script type="math/tex; mode=display">
\text{CV}(\text{Importance}(X)) = \frac{\text{Std}(\text{Importance}(X))}{\text{Mean}(\text{Importance}(X))}</script><blockquote>
<p> 高 CV 值表示专家利用的不平衡性较大，低 CV 值表示专家利用较为均衡。</p>
</blockquote>
</li>
<li><p><strong>定义额外损失项 $L_{\text{importance}}$：</strong></p>
<p>为了最小化专家利用的变异性，定义损失项为变异系数的平方，再乘以一个缩放因子 $w_{\text{importance}}$：</p>
<script type="math/tex; mode=display">
L_{\text{importance}}(X) = w_{\text{importance}} \cdot \text{CV}(\text{Importance}(X))^2</script><p>这个损失项的目标是 <strong>最小化专家利用的变异系数</strong>，即鼓励专家利用的均衡性。</p>
</li>
<li><p><strong>总体损失函数：</strong></p>
<p>将额外的损失项加入到模型的总体损失函数中：</p>
<script type="math/tex; mode=display">
L_{\text{total}} = L_{\text{task}} + L_{\text{importance}}</script><p>其中，$L_{\text{task}}$ 是<strong>原始的任务损失</strong>（如语言建模的交叉熵损失）。</p>
<h4 id="为什么额外损失项可以解决专家利用不平衡的问题？"><a href="#为什么额外损失项可以解决专家利用不平衡的问题？" class="headerlink" title="为什么额外损失项可以解决专家利用不平衡的问题？"></a><strong>为什么额外损失项可以解决专家利用不平衡的问题？</strong></h4><p>引入 $L_{\text{importance}}$ 的原因和作用可以从以下几个方面理解：</p>
</li>
<li><p><strong>惩罚不平衡性：</strong></p>
<p>$L_{\text{importance}}$ 随着专家利用的不平衡性增加而增加。这迫使模型在训练过程中不仅要优化任务损失，还要尽量保持各专家的利用率一致。</p>
</li>
<li><p><strong>鼓励均衡选择：</strong></p>
<p>通过最小化变异系数，模型被鼓励在选择专家时更加均衡，避免过度依赖某些专家。这有助于所有专家都有机会参与到训练和推理中，充分发挥各自的潜力。</p>
</li>
<li><p><strong>防止自我强化（Self-Reinforcement）：</strong></p>
<p>如果没有平衡机制，门控网络可能会倾向于选择表现最好的专家，从而使这些专家进一步优化并被更多选择，形成自我强化的循环。而 $L_{\text{importance}}$ 破坏了这种循环，迫使模型在选择专家时考虑整体的利用平衡。</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Relativity suis</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">http://example.com/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Relativity suis's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MoE/">MoE</a></div><div class="post-share"><div class="social-share" data-image="/img/486.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/08/MoE-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="MoE 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">MoE 论文研读</div></div><div class="info-2"><div class="info-item-1">前两篇基础且经典的 MoE 工作可见： Adaptive Mixtures of Local Experts 论文研读 | Relativity suis’s Blog Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读 | Relativity suis’s Blog GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingModelSparse scaling of the Transformer architecture首先简单回顾 Transformer 结构：  Transformer 编码器层由两个连续的层组成，即自注意力层和逐位置前馈层。解码器在此基础上增加了第三个交叉注意力层，该层会对编码器的输出进行关注。  作者通过条件计算对 Transformer...</div></div></div></a><a class="pagination-related" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Adaptive Mixtures of Local Experts 论文研读</div></div><div class="info-2"><div class="info-item-1">Adaptive Mixtures of Local Experts 论文研读论文链接：https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf 参考链接：https://zhuanlan.zhihu.com/p/423447025 Abstract We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-07</div><div class="info-item-2">Adaptive Mixtures of Local Experts 论文研读</div></div><div class="info-2"><div class="info-item-1">Adaptive Mixtures of Local Experts 论文研读论文链接：https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf 参考链接：https://zhuanlan.zhihu.com/p/423447025 Abstract We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive...</div></div></div></a><a class="pagination-related" href="/2025/01/08/MoE-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="MoE 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-08</div><div class="info-item-2">MoE 论文研读</div></div><div class="info-2"><div class="info-item-1">前两篇基础且经典的 MoE 工作可见： Adaptive Mixtures of Local Experts 论文研读 | Relativity suis’s Blog Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读 | Relativity suis’s Blog GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingModelSparse scaling of the Transformer architecture首先简单回顾 Transformer 结构：  Transformer 编码器层由两个连续的层组成，即自注意力层和逐位置前馈层。解码器在此基础上增加了第三个交叉注意力层，该层会对编码器的输出进行关注。  作者通过条件计算对 Transformer...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/486.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Relativity suis</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Gcy-shili"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Blog 积极更新中！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB"><span class="toc-number">1.</span> <span class="toc-text">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Approach%EF%BC%9ATHE-SPARSELY-GATED-MIXTURE-OF-EXPERTS-LAYER"><span class="toc-number">1.2.</span> <span class="toc-text">The Approach：THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Structure-of-the-MoE-Layer"><span class="toc-number">1.3.</span> <span class="toc-text">The Structure of the MoE Layer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gating-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">Gating Network</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BALANCING-EXPERT-UTILIZATION"><span class="toc-number">1.4.</span> <span class="toc-text">BALANCING EXPERT UTILIZATION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">一些问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%981%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8E%A7%E5%88%B6%E9%97%A8%E6%8E%A7%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%A8%80%E7%96%8F%E7%9A%84%E6%9D%83%E9%87%8D%E5%90%91%E9%87%8F%EF%BC%8C%E5%85%B7%E4%BD%93%E6%9D%A5%E8%AF%B4-Noisy-Top-K-%E9%97%A8%E6%8E%A7%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%BF%99%E4%B8%80%E7%82%B9%E7%9A%84%EF%BC%9F"><span class="toc-number">1.5.1.</span> <span class="toc-text">问题1：如何控制门控网络输出为一个稀疏的权重向量，具体来说 Noisy Top-K 门控是如何实现这一点的？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E7%A8%80%E7%96%8F%E6%9D%83%E9%87%8D%E5%90%91%E9%87%8F"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">理解稀疏权重向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Noisy-Top-K-%E9%97%A8%E6%8E%A7%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">Noisy Top-K 门控的实现步骤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%982%EF%BC%9A%E5%8F%AF%E8%B0%83%E7%9A%84%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%BB%E5%8A%A0%E5%AE%83%E5%8F%AF%E4%BB%A5%E5%A2%9E%E5%8A%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%EF%BC%9F"><span class="toc-number">1.5.2.</span> <span class="toc-text">问题2：可调的高斯噪声是什么，为什么添加它可以增加模型的鲁棒性？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E8%B0%83%E7%9A%84%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">可调的高斯噪声的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%BB%E5%8A%A0%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E5%8F%AF%E4%BB%A5%E5%A2%9E%E5%8A%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%EF%BC%9F"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">为什么添加高斯噪声可以增加模型的鲁棒性？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%9C%BA%E5%88%B6%E8%A7%A3%E9%87%8A"><span class="toc-number">1.5.2.3.</span> <span class="toc-text">具体机制解释</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%983%EF%BC%9A%E5%9C%A8%E5%B9%B3%E8%A1%A1%E4%B8%93%E5%AE%B6%E5%88%A9%E7%94%A8%E4%B8%AD%E5%BC%95%E5%85%A5%E7%9A%84%E9%A2%9D%E5%A4%96%E6%8D%9F%E5%A4%B1%E9%A1%B9%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E4%B8%93%E5%AE%B6%E5%88%A9%E7%94%A8%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">1.5.3.</span> <span class="toc-text">问题3：在平衡专家利用中引入的额外损失项为什么可以解决专家利用不平衡的问题？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%93%E5%AE%B6%E5%88%A9%E7%94%A8%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">专家利用不平衡的问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E9%A2%9D%E5%A4%96%E6%8D%9F%E5%A4%B1%E9%A1%B9%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">引入额外损失项的目的</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.5.3.3.</span> <span class="toc-text">具体实现步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%A2%9D%E5%A4%96%E6%8D%9F%E5%A4%B1%E9%A1%B9%E5%8F%AF%E4%BB%A5%E8%A7%A3%E5%86%B3%E4%B8%93%E5%AE%B6%E5%88%A9%E7%94%A8%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">1.5.3.4.</span> <span class="toc-text">为什么额外损失项可以解决专家利用不平衡的问题？</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/08/LoRA-%E5%8F%8A%E5%85%B6%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="LoRA 及其论文研读">LoRA 及其论文研读</a><time datetime="2025-01-08T12:47:34.000Z" title="发表于 2025-01-08 20:47:34">2025-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/08/MoE-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="MoE 论文研读">MoE 论文研读</a><time datetime="2025-01-08T08:27:49.000Z" title="发表于 2025-01-08 16:27:49">2025-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</a><time datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读">Adaptive Mixtures of Local Experts 论文研读</a><time datetime="2025-01-06T16:29:09.000Z" title="发表于 2025-01-07 00:29:09">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/RoPE/" title="RoPE">RoPE</a><time datetime="2025-01-05T16:16:07.000Z" title="发表于 2025-01-06 00:16:07">2025-01-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Relativity suis</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">"Suis is all you need"</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>