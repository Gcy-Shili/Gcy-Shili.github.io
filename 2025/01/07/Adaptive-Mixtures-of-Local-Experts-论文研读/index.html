<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Adaptive Mixtures of Local Experts 论文研读 | Relativity suis's Blog</title><meta name="author" content="Relativity suis"><meta name="copyright" content="Relativity suis"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Adaptive Mixtures of Local Experts 论文研读论文链接：https:&#x2F;&#x2F;people.engr.tamu.edu&#x2F;rgutier&#x2F;web_courses&#x2F;cpsc636_s10&#x2F;jacobs1991moe.pdf 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;423447025 Abstract We present a new supervi">
<meta property="og:type" content="article">
<meta property="og:title" content="Adaptive Mixtures of Local Experts 论文研读">
<meta property="og:url" content="http://example.com/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html">
<meta property="og:site_name" content="Relativity suis&#39;s Blog">
<meta property="og:description" content="Adaptive Mixtures of Local Experts 论文研读论文链接：https:&#x2F;&#x2F;people.engr.tamu.edu&#x2F;rgutier&#x2F;web_courses&#x2F;cpsc636_s10&#x2F;jacobs1991moe.pdf 参考链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;423447025 Abstract We present a new supervi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/486.jpg">
<meta property="article:published_time" content="2025-01-06T16:29:09.000Z">
<meta property="article:modified_time" content="2025-01-07T17:31:01.571Z">
<meta property="article:author" content="Relativity suis">
<meta property="article:tag" content="MoE">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/486.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Adaptive Mixtures of Local Experts 论文研读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/486.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/suis1.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Relativity suis's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Adaptive Mixtures of Local Experts 论文研读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Adaptive Mixtures of Local Experts 论文研读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-06T16:29:09.000Z" title="发表于 2025-01-07 00:29:09">2025-01-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-07T17:31:01.571Z" title="更新于 2025-01-08 01:31:01">2025-01-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Adaptive-Mixtures-of-Local-Experts-论文研读"><a href="#Adaptive-Mixtures-of-Local-Experts-论文研读" class="headerlink" title="Adaptive Mixtures of Local Experts 论文研读"></a>Adaptive Mixtures of Local Experts 论文研读</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf">https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf</a></p>
<p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/423447025">https://zhuanlan.zhihu.com/p/423447025</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>We present a new supervised learning procedure for <strong>systems composed of many separate networks</strong>, each of which learns to handle <strong>a subset of the complete set of training cases</strong>. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimi-nation task into appropriate subtasks, each of which can be solved by a very simple expert network.</p>
</blockquote>
<p>我们提出了一种新的监督学习过程，适用于<strong>由多个独立网络组成的系统</strong>，每个网络学习处理<strong>完整训练案例集中的一部分</strong>（一个子集）。这一新过程既可以视为多层监督网络的模块化版本，也可以看作是竞争学习的关联版本。因此，它在两种看似不同的方法之间建立了新的联系。我们证明了该学习过程能够将元音辨别任务分解为适当的子任务，每个子任务都可以由一个非常简单的专家网络解决。</p>
<hr>
<h2 id="Making-Associative-Learning-Competitive"><a href="#Making-Associative-Learning-Competitive" class="headerlink" title="Making Associative Learning Competitive"></a>Making Associative Learning Competitive</h2><p>对于传统的学习模型来说，训练的主要目的是使模型最终能够在不同的场景下执行多种任务，但这种训练方式也使得模型在对相应场景进行权重更新的同时，也会影响到模型对其它场景的权重。文章中提到：</p>
<blockquote>
<p>若采用反向传播算法训练一个单一的多层网络，使其在不同场合执行不同的子任务，通常会产生强烈的<strong>干扰效应</strong>（interference effects），导致<strong>学习速度缓慢和泛化能力差</strong>（lead to slow learning and poor generalization）。</p>
</blockquote>
<p>因此，如果我们预先知道一组训练案例可以自然地划分为对应于不同子任务的子集，那么干扰效应就可以通过使用一个由多个不同的 “专家” 网络（Expert networks）和一个门控网络（gating network）组成的系统被减弱，其中门控网络决定每个训练案例应该使用哪个专家网络。</p>
<p>接着作者描述了前人研究的两种系统：</p>
<p>①Hampshire 和 Waibel（1989年）描述了一种这样的系统，它可以在<strong>训练前已知子任务划分</strong>的情况下使用；</p>
<p>②Jacobs 等人（1990年）描述了一种相关的系统，该系统学习如何将案例分配给专家，这种系统的核心思想是，门控网络将新案例分配给一个或少数几个专家，如果输出不正确，则权重的调整仅限于这些被分配过案例的专家（以及门控网络）。因此，不会干扰到专门处理完全不同案例的其他专家的权重。从这个意义上说，专家是局部的，因为一个专家的权重与其他专家的权重是解耦的。此外，专家通常在另一种意义上也是局部的，即每个专家只被分配到可能的输入向量空间的一个小的局部区域。</p>
<p>但是，Hampshire 和 Waibel 以及 Jacobs 等人所使用的误差函数并未促进局部化。他们假设整个系统的最终输出是局部专家输出的<strong>线性组合</strong>（linear combination of the outputs of the local experts），而门控网络则决定了每个局部输出在线性组合中的比例。所以对于一个案例 $c$ 的误差函数为：</p>
<script type="math/tex; mode=display">
E^{c}\,=\,\|{\bf d}^{c}-\sum_{i}{p}_{i}^{c}{\bf o}_{i}^{c}\,\|^{2} \tag{1.1}</script><p>其中，${\bf o}<em>{i}^{c}$ 是专家 $i$ 在案例 $c$ 中的输出向量，$p</em>{i}^{c}$ 是专家 $i$ 对（线性）组合输出向量的贡献比例，${\bf d}^c$ 是案例 $c$ 中期望的输出向量。</p>
<p>上述误差的度量是<strong>将期望输出与局部专家输出的混合结果进行比较</strong>，因此，为了最小化误差，每个局部专家必须使他们的输出抵消由所有其他专家的联合效应留下的残差。当一个专家的权重发生变化时，残差也会变化，因此所有其他局部专家的误差导数也会变化。这种专家之间的强耦合使它们能够很好地合作，<strong>但往往会导致每个案例使用多个专家的解决方案</strong>。可以通过在目标函数中添加惩罚项来<strong>鼓励竞争</strong>，以鼓励只有一个专家活跃的解决方案（Jacobs等，1990年），但更简单的补救方法是重新定义误差函数，以鼓励局部专家竞争而不是合作。</p>
<p>而作者的工作不是将各个专家的输出进行线性组合，而是设想门控网络在每次使用时随机决定使用哪个单一专家，误差则是期望输出向量与实际输出向量之间差异平方的期望值：</p>
<script type="math/tex; mode=display">
E^{c}=\langle\|\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\|^{2}\rangle=\sum_{i}p_{ i}^{c}\|\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\|^{2} \tag{1.2}</script><p>在这个新的误差函数中，每个专家需要生成整个输出向量，而不仅仅是残差。因此，给定训练案例中局部专家的目标不会直接受到其他局部专家权重的影响。仍然存在一些间接耦合，因为如果其他专家改变了其权重，可能会导致门控网络改变分配给专家的责任，但至少这些责任的变化不会改变局部专家在给定训练案例中感知到的误差符号。如果门控网络和局部专家都是通过梯度下降法在这个新的误差函数中进行训练，系统往往会为每个训练案例分配一个专家。每当一个专家的误差小于所有专家误差的加权平均值（使用门控网络的输出来决定如何加权每个专家的误差）时，它对该案例的责任将会增加；而当它的表现比加权平均值差时，其责任将会减少。</p>
<p><img src="/images/moe91-1.png" alt="moe91-1"></p>
<blockquote>
<p> 图1：一个由专家网络和门控网络组成的系统。每个专家都是一个前馈网络，所有专家接收相同的输入并具有相同数量的输出。门控网络也是前馈的，通常接收与专家网络相同的输入。它的输出经过归一化处理，即 $p<em>j = \frac{\exp(x_j)}{\sum</em>{i} \exp(x_i)}$，其中 $x_j$ 是门控网络输出单元 $j$ 接收到的总加权输入。选择器就像一个多输入、单输出的随机开关；开关选择专家 $j$ 的输出的概率为 $p_j$。</p>
</blockquote>
<p>上述新的误差函数在实践中有效，但在下面的模拟中作者使用了另一个误差函数，效果更好：</p>
<script type="math/tex; mode=display">
E^{c} = -\log\sum_{i} p_{i}^{c}e^{-\frac{1}{2}||\mathbf{d}^{c}- \mathbf{o}_{i}^{c}||^{2}} \tag{1.3}</script><p>以上定义的误差是在下一节末尾描述的高斯混合模型下生成期望输出向量的负对数概率，为了理解为什么这个误差函数效果更好，比较两个误差函数对专家输出的导数是有帮助的。由方程 1.2 可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial{E^c}}{\partial{\mathbf{o}_i^c}} = -2p_i^c({\bf d}^c-\mathbf{o}_i^c) \tag{1.4}</script><p>由方程 1.3：</p>
<script type="math/tex; mode=display">
\frac{\partial E^c}{\partial \mathbf{o}_i^c} = -\left[\frac{p_i^c e^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_i^c\|^2}}{\sum_j p_j^c e^{-\frac{1}{2}\|\mathbf{d}^c-\mathbf{o}_j^c\|^2}}\right](\mathbf{d}^c - \mathbf{o}_i^c)  \tag{1.5}</script><p>①在方程 1.4 中，项 $p_i^c$ 用于为专家 $i$ 的导数加权；</p>
<p>②在方程 1.5 中，我们使用了一个权重项，该项考虑了专家 $i$ 相对于其他专家的表现程度。这是一个更有用的衡量专家 $i$ 对训练案例 $c$ 的相关性的指标，特别是在训练的早期阶段。例如，假设门控网络最初给所有专家赋予相等的权重，且对所有专家来说 $||{\bf d}^c - {\bf o}_i^c|| &gt; 1$。方程 1.4 将最慢地调整最佳拟合专家，而方程 1.5 将最快地调整它。</p>
<h2 id="Making-Competitive-Learning-Associative"><a href="#Making-Competitive-Learning-Associative" class="headerlink" title="Making Competitive Learning Associative"></a>Making Competitive Learning Associative</h2><p>自然地，我们会认为竞争网络训练的“数据”向量类似于关联网络的输入向量，这些输入向量被映射到输出向量。在使用竞争学习作为关联网络预处理阶段的模型中，这种对应关系被假定存在（Moody 和 Darken 1989）。然而，另一种截然不同的观点认为，竞争学习中的数据向量对应于关联网络的输出向量。在这种情况下，竞争网络可以被视为一个无输入的随机输出向量生成器，而竞争学习则可以被视为一种使网络生成与“数据”向量分布相匹配的输出向量分布的过程。每个竞争隐藏单元的权重向量代表了一个多维高斯分布的均值，输出向量的生成过程首先是通过选择一个隐藏单元，然后从由该隐藏单元的权重向量确定的高斯分布中选择一个输出向量。生成任意特定输出向量 ${\bf o}^c$ 的对数概率：</p>
<script type="math/tex; mode=display">
\log P^c = \log \sum_i p_{i}ke^{-\frac{1}{2}\|\boldsymbol{\mu}_i-\mathbf{o}^c\|^2}</script><p>其中 $i$ 是隐藏单元的索引，$\boldsymbol{\mu}_i$ 是隐藏单元的”权重”向量，$k$ 是归一化常数，$p_i$ 是选择隐藏单元 $i$ 的概率，因此 $p_i$ 之和受约束等于1。在统计文献中（McLachlan and Basford 1988），$p_i$ 被称为”混合比例”。</p>
<p>“软”竞争学习通过修改权重（以及方差和混合比例）来增加生成训练集中输出向量的概率的乘积（即似然度）(Nowlan 1990a)。”硬”竞争学习是软竞争学习的一个简单近似，它忽略了数据向量可能由几个不同隐藏单元生成的可能性。相反，我们假设数据向量必须由具有最接近权重向量的隐藏单元生成，因此只需要修改这个权重向量来增加生成数据向量的概率。</p>
<p>如果我们将竞争网络视为生成输出向量的系统，输入向量可能扮演的角色并不是立即显而易见的。然而，竞争学习可以以与Barto(1985)泛化学习自动机相似的方式进行泛化，即通过添加输入向量并使自动机的行为依赖于输入向量。我们用整个专家网络替换竞争网络中的每个隐藏单元，其输出向量指定了多维高斯分布的均值。因此，均值现在是当前输入向量的函数，并且由活动水平而不是权重来表示。此外，我们使用一个门控网络，它允许专家的混合比例由输入向量决定。这给了我们一个由局部专家组成的竞争系统，其误差函数在等式1.3中定义。我们也可以引入一个机制，允许输入向量动态确定每个专家网络定义的分布的协方差矩阵，但我们还没有尝试过这种可能性。</p>
<hr>
<h2 id="一些理解"><a href="#一些理解" class="headerlink" title="一些理解"></a>一些理解</h2><h3 id="一、背景与动机"><a href="#一、背景与动机" class="headerlink" title="一、背景与动机"></a>一、背景与动机</h3><p>在传统的监督学习中，我们常用一个多层神经网络来处理各种任务。然而，当同一个网络需要学习处理多种不同的子任务时，会出现<strong>干扰效应</strong>（interference effects）：在训练网络以适应一个场景时，不可避免地会影响到它在其他场景下的表现。这种干扰会导致学习速度变慢，泛化能力变差。<br><strong>问题</strong>：如何设计一个系统，使得每个子任务由专门的“专家网络”来处理，从而减少不同任务之间的干扰？<br><strong>解决思路</strong>：引入多个独立的专家网络，每个专家专注于处理一小部分任务或特定的输入模式，再通过一个门控网络决定使用哪个专家。这种架构被称为 “混合专家模型”（Mixture of Experts）。</p>
<h3 id="二、混合专家模型的基本结构"><a href="#二、混合专家模型的基本结构" class="headerlink" title="二、混合专家模型的基本结构"></a>二、混合专家模型的基本结构</h3><p>模型主要由两部分组成：</p>
<ol>
<li><strong>专家网络（Expert Networks）</strong>：多个并行的子网络，各自独立处理不同的任务或数据子集。</li>
<li><strong>门控网络（Gating Network）</strong>：一个网络，根据输入数据决定哪个专家（或哪些专家）的输出最适合当前任务。<br>在每次处理一个输入时，门控网络会评估哪个专家最有可能给出正确的结果，并选择相应的专家来生成输出。</li>
</ol>
<hr>
<h3 id="三、传统方法的问题与改进方向"><a href="#三、传统方法的问题与改进方向" class="headerlink" title="三、传统方法的问题与改进方向"></a>三、传统方法的问题与改进方向</h3><h4 id="传统的误差函数与专家组合"><a href="#传统的误差函数与专家组合" class="headerlink" title="传统的误差函数与专家组合"></a>传统的误差函数与专家组合</h4><p>过去的一种常见方法是让整个系统的输出作为各个专家输出的<strong>线性组合</strong>。形式上，对于某个训练样本 $c$，误差函数定义为：</p>
<script type="math/tex; mode=display">
E^{c} = \Big\| \mathbf{d}^{c} - \sum_{i} p_i^c \mathbf{o}_i^c \Big\|^2</script><ul>
<li>$\mathbf{d}^c$：期望的输出向量</li>
<li>$\mathbf{o}_i^c$：专家 $i$ 对应样本 $c$ 的输出</li>
<li>$p_i^c$：门控网络给予专家 $i$ 的权重（对应其在输出组合中的贡献比例）<br><strong>问题</strong>：这种设置下，各个专家需要协同工作来共同逼近期望输出。这意味着：</li>
<li>当一个专家调整权重时，会影响剩余专家<strong>需要补偿的残差</strong>，从而间接影响其它专家的工作。</li>
<li>导致多个专家可能都参与到一个样本的处理上，增加了干扰和复杂性。<h4 id="改进方向：鼓励竞争而非合作"><a href="#改进方向：鼓励竞争而非合作" class="headerlink" title="改进方向：鼓励竞争而非合作"></a>改进方向：鼓励竞争而非合作</h4>为了解决上述问题，作者提出改变误差函数，使专家之间<strong>竞争</strong>而非合作。这样，每个训练案例最终倾向于分配给一个最合适的专家，而不是多个专家共同处理。</li>
</ul>
<h3 id="四、第一种实现思路-——-使关联学习具有竞争性"><a href="#四、第一种实现思路-——-使关联学习具有竞争性" class="headerlink" title="四、第一种实现思路 —— 使关联学习具有竞争性"></a>四、第一种实现思路 —— 使关联学习具有竞争性</h3><p><strong>核心思想</strong>：重新定义误差函数，让门控网络在每次选择时随机决定使用哪个单一的专家，而不是线性组合多个专家的输出。<br>新的误差函数定义为：</p>
<script type="math/tex; mode=display">
E^{c} = \sum_{i} p_{i}^{c}\,\|\mathbf{d}^{c}-\mathbf{o}_{i}^{c}\|^{2}</script><p>其中：</p>
<ul>
<li>仍然保持 $p_i^c$ 代表门控网络选择专家 $i$ 的概率。</li>
<li>这个误差函数表示：对于每个可能的专家 $i$，以其被选择的概率加权，它单独产生的输出与期望输出之间的误差。<br><strong>效果</strong>：</li>
<li>每个专家独立地尝试对整个输出负责，而不只是去补偿其他专家留下的残差。</li>
<li>当某个专家在处理某个样本时表现优于其他专家，它获得的“责任”会增加，从而更多地参与到类似样本的学习中。</li>
</ul>
<p>为了进一步改进效果，作者提出了一个变体的误差函数：</p>
<script type="math/tex; mode=display">
E^{c} = -\log\sum_{i} p_{i}^{c}e^{-\frac{1}{2}\|\mathbf{d}^{c}- \mathbf{o}_{i}^{c}\|^{2}}</script><p>此公式的优势在于：</p>
<ul>
<li>在计算梯度时，会自然地更快地调整表现最佳的专家，而不是平均调整所有专家。</li>
<li>这通过一个<strong>softmax</strong>风格的权重机制，自动放大那些与期望输出更接近的专家的影响，鼓励更快地专精化。<br><strong>具体效果</strong>：</li>
<li>在训练早期，当所有专家的表现都较差时，新误差函数能更快地找到并调整最有潜力的专家。</li>
<li>这种机制减少了多个专家在同一个任务上的不必要竞争，使得一个专家更快地“专精”于某些数据区域。</li>
</ul>
<hr>
<h3 id="五、第二种实现思路-——-使竞争学习具有联想性"><a href="#五、第二种实现思路-——-使竞争学习具有联想性" class="headerlink" title="五、第二种实现思路 —— 使竞争学习具有联想性"></a>五、第二种实现思路 —— 使竞争学习具有联想性</h3><p><strong>背景</strong>：竞争学习通常用于无监督聚类，比如找到数据中的典型模式或中心（如聚类中心）。在传统的竞争学习中，每个隐藏单元代表一个聚类中心，数据点被分配到距离最近的中心。<br><strong>转换思路</strong>：将竞争学习的思想引入到关联（即输入输出映射）的情境中，把竞争网络看作一个<strong>生成输出向量的系统</strong>，而不仅仅是对输入聚类。</p>
<ol>
<li><p><strong>概率模型视角</strong>：</p>
<ul>
<li>假设每个隐藏单元对应一个多维高斯分布，均值由该单元的权重向量决定。</li>
<li><p>给定一个输出向量 $\mathbf{o}^c$，其生成概率可以表示为高斯混合模型的形式：</p>
<script type="math/tex; mode=display">
\log P^c = \log \sum_i p_{i}k e^{-\frac{1}{2}\|\boldsymbol{\mu}_i - \mathbf{o}^c\|^2}</script><ul>
<li>$\boldsymbol{\mu}_i$：隐藏单元 $i$ 的权重向量，相当于高斯分布的均值</li>
<li>$p_i$：选择隐藏单元 $i$ 的概率，称为混合比例</li>
</ul>
</li>
</ul>
</li>
<li><strong>软硬竞争学习</strong>：<ul>
<li><strong>软竞争学习</strong>：根据整个概率分布调整所有单元的权重，使得训练数据的似然度最大化。</li>
<li><strong>硬竞争学习</strong>：简化处理，只调整对给定数据点贡献最大的那个单元的权重。</li>
</ul>
</li>
<li><strong>关联性引入</strong>：<ul>
<li>将每个竞争学习的隐藏单元替换为一个专家网络，使得该网络的输出（而非固定权重）决定高斯分布的均值。</li>
<li>引入门控网络，使得混合比例 $p_i$ 依赖于输入向量，而不再是常数。</li>
<li>结果是一个输入-输出映射系统：给定输入后，门控网络决定调用哪个专家网络，而该专家网络生成输出。</li>
</ul>
</li>
</ol>
<p><strong>意义</strong>：</p>
<ul>
<li>这种设置把传统的竞争学习（无监督）与监督学习结合起来，构建了一个能够根据输入生成适当输出的系统。</li>
<li>专家网络不仅仅是简单的固定聚类中心，而是可以根据输入动态调整输出，提供更灵活的映射能力。</li>
</ul>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><blockquote>
<p>问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？<br>问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？<br>问题3：这种网络是如何进行训练的？<br>问题4：门控网络如何决定调用哪个专家网络？</p>
</blockquote>
<h3 id="问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？"><a href="#问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？" class="headerlink" title="问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？"></a><strong>问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？</strong></h3><p><strong>理解传统方法中的“残差补偿”</strong><br><strong>背景回顾：</strong><br>在<strong>传统</strong>的混合专家模型中，系统的最终输出是各个专家网络输出的线性组合。具体来说，对于一个训练样本 $c$，系统的输出是：</p>
<script type="math/tex; mode=display">
\mathbf{O}^c = \sum_{i} p_i^c \mathbf{o}_i^c</script><p>其中：</p>
<ul>
<li>$\mathbf{o}_i^c$ 是专家 $i$ 对样本 $c$ 的输出。</li>
<li>$p_i^c$ 是门控网络为专家 $i$ 分配的权重（概率）。</li>
</ul>
<p><strong>误差函数：</strong><br>误差定义为期望输出 $\mathbf{d}^c$ 与系统输出 $\mathbf{O}^c$ 之间的差距：</p>
<script type="math/tex; mode=display">
E^{c} = \| \mathbf{d}^c - \mathbf{O}^c \|^2 = \left\| \mathbf{d}^c - \sum_{i} p_i^c \mathbf{o}_i^c \right\|^2</script><p><strong>残差补偿的含义：</strong></p>
<ul>
<li><strong>残差（Residual）：</strong> 指的是当前系统输出与期望输出之间的差距，即 $\mathbf{d}^c - \mathbf{O}^c$。</li>
<li><strong>补偿残差：</strong> 每个专家网络的输出 $\mathbf{o}_i^c$ 都在尝试减小这个残差，使系统输出更接近期望输出。</li>
</ul>
<p><strong>为何调整一个专家影响其他专家：</strong></p>
<ol>
<li><strong>线性组合的依赖性：</strong> 因为系统输出是所有专家输出的加权和，改变某一个专家的输出 $\mathbf{o}_i^c$ 会直接影响 $\mathbf{O}^c$。</li>
<li><strong>残差的变化：</strong> 当某个专家调整了其输出 $\mathbf{o}_i^c$，整个系统的残差 $\mathbf{d}^c - \mathbf{O}^c$ 也会相应变化。</li>
<li><strong>其他专家的响应：</strong> 由于残差变化，其他专家需要调整它们的输出以重新补偿新的残差，以维持系统输出的准确性。</li>
</ol>
<p><strong>举个简单的例子：</strong><br>假设有两个专家 A 和 B：</p>
<ul>
<li>系统输出 $\mathbf{O} = p_A \mathbf{o}_A + p_B \mathbf{o}_B$。</li>
<li>初始时，A 和 B 都有一定的输出，系统输出接近期望输出。</li>
</ul>
<p>如果专家 A 调整了其输出 $\mathbf{o}_A$（例如增加了输出值），那么系统输出 $\mathbf{O}$ 会增加，从而引入一个新的残差（假设期望输出保持不变）。为了减小新的残差，专家 B 需要调整其输出 $\mathbf{o}_B$ 来补偿 A 的变化。这样，A 的调整直接导致了 B 的响应，形成了专家之间的<strong>强耦合</strong>。</p>
<h3 id="问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？"><a href="#问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？" class="headerlink" title="问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？"></a><strong>问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？</strong></h3><p><strong>理解误差函数变体及其梯度对训练效果的影响</strong><br><strong>原始误差函数 vs. 变体误差函数：</strong></p>
<ul>
<li><p><strong>原始误差函数：</strong></p>
<script type="math/tex; mode=display">
E^{c} = \left\| \mathbf{d}^c - \sum_{i} p_i^c \mathbf{o}_i^c \right\|^2</script></li>
<li><p><strong>变体误差函数：</strong></p>
<script type="math/tex; mode=display">
E^{c} = -\log\left( \sum_{i} p_i^c e^{-\frac{1}{2} \| \mathbf{d}^c - \mathbf{o}_i^c \|^2} \right)</script></li>
</ul>
<p><strong>为什么比较导数有助于判断效果：</strong></p>
<ul>
<li><strong>梯度下降法：</strong> 在训练神经网络时，我们通常使用梯度下降法来最小化误差函数。梯度（导数）决定了参数更新的方向和幅度。</li>
<li><strong>影响训练过程：</strong> 误差函数的梯度影响模型如何调整参数以减少误差。不同的误差函数会导致不同的梯度，从而影响模型的学习速度和收敛效果。</li>
</ul>
<p><strong>比较导数的具体方式：</strong></p>
<ol>
<li><p><strong>计算各自的梯度：</strong></p>
<ul>
<li><p><strong>原始误差函数的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial E^c}{\partial \mathbf{o}_i^c} = -2 p_i^c (\mathbf{d}^c - \mathbf{o}_i^c)</script><ul>
<li><strong>解释：</strong> 每个专家的输出梯度与其分配的权重 $p_i^c$ 和输出误差 $(\mathbf{d}^c - \mathbf{o}_i^c)$ 成正比。</li>
</ul>
</li>
<li><p><strong>变体误差函数的梯度：</strong></p>
<script type="math/tex; mode=display">
\frac{\partial E^c}{\partial \mathbf{o}_i^c} = - \left[ \frac{p_i^c e^{-\frac{1}{2} \| \mathbf{d}^c - \mathbf{o}_i^c \|^2}}{\sum_j p_j^c e^{-\frac{1}{2} \| \mathbf{d}^c - \mathbf{o}_j^c \|^2}} \right] (\mathbf{d}^c - \mathbf{o}_i^c)</script><ul>
<li><strong>解释：</strong> 每个专家的梯度不仅取决于其分配的权重和输出误差，还受到一个<strong>额外的归一化因子的调节</strong>，这个因子反映了专家相对于其他专家的表现。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>分析梯度的影响：</strong></p>
<ul>
<li><p><strong>原始误差函数：</strong> 所有专家都会根据它们的权重和误差调整输出。即使某些专家的表现较差（误差大），它们也会对梯度有贡献，但这种贡献是平均的。</p>
</li>
<li><p><strong>变体误差函数：</strong> 梯度的贡献被加权，表现较好的专家（误差较小，靠近期望输出）的梯度贡献相对较大，而表现较差的专家的贡献较小。具体来说，变体误差函数的梯度对表现好的专家更敏感，促进这些专家更快地调整以进一步优化输出。</p>
</li>
</ul>
</li>
<li><p><strong>效果上的优势：</strong></p>
<ul>
<li><strong>加速最佳专家的调整：</strong> 由于变体误差函数在梯度中对表现好的专家赋予更大的权重，这些专家能够更快地调整和优化，迅速适应特定的子任务或数据模式。</li>
<li><strong>鼓励专家间的竞争：</strong> 变体误差函数自然地促进了专家之间的竞争，优秀的专家会得到更多的关注和资源，而表现差的专家则会逐渐被淘汰或调整以适应更合适的任务。</li>
</ul>
</li>
</ol>
<h3 id="问题3：这种网络是如何进行训练的？"><a href="#问题3：这种网络是如何进行训练的？" class="headerlink" title="问题3：这种网络是如何进行训练的？"></a><strong>问题3：这种网络是如何进行训练的？</strong></h3><p><strong>理解混合专家模型的训练流程</strong><br><strong>模型组成：</strong></p>
<ol>
<li><strong>专家网络（Expert Networks）：</strong> 多个独立的子网络，每个负责处理特定的子任务或数据子集。</li>
<li><strong>门控网络（Gating Network）：</strong> 一个网络，用于根据输入数据决定使用哪个专家（或哪些专家）。</li>
</ol>
<p><strong>训练步骤：</strong></p>
<ol>
<li><p><strong>初始化：</strong></p>
<ul>
<li><strong>参数初始化：</strong> 随机初始化所有专家网络和门控网络的参数（权重和偏置）。</li>
</ul>
</li>
<li><p><strong>前向传播（Forward Pass）：</strong></p>
<ul>
<li><strong>输入处理：</strong> 对于每一个训练样本 $c$：<ul>
<li>将输入向量 $\mathbf{x}^c$ 传递给<strong>所有专家网络</strong>，得到各自的输出 $\mathbf{o}_i^c$。</li>
<li>将输入向量 $\mathbf{x}^c$ 传递给门控网络，得到各专家的选择概率 $p_i^c$（通常通过 Softmax 函数归一化）。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>输出组合：</strong></p>
<ul>
<li><strong>系统输出：</strong> 计算系统的最终输出 $\mathbf{O}^c = \sum_{i} p_i^c \mathbf{o}_i^c$。</li>
</ul>
</li>
<li><p><strong>误差计算：</strong></p>
<ul>
<li><p><strong>原始误差函数：</strong></p>
<script type="math/tex; mode=display">
E^{c} = \left\| \mathbf{d}^c - \mathbf{O}^c \right\|^2</script></li>
<li><p><strong>变体误差函数：</strong></p>
<script type="math/tex; mode=display">
E^{c} = -\log \left( \sum_{i} p_i^c e^{-\frac{1}{2} \| \mathbf{d}^c - \mathbf{o}_i^c \|^2} \right)</script></li>
</ul>
</li>
<li><p><strong>反向传播（Backward Pass）：</strong></p>
<ul>
<li><p><strong>计算梯度：</strong> 根据选定的误差函数，计算所有专家网络和门控网络参数的梯度。</p>
</li>
<li><p><strong>更新参数：</strong> 使用梯度下降或其变体（如随机梯度下降、Adam 等）更新所有网络的参数：</p>
<script type="math/tex; mode=display">
\theta \leftarrow \theta - \eta \frac{\partial E^c}{\partial \theta}</script><p>其中 $\theta$ 代表所有网络的参数，$\eta$ 是学习率。</p>
</li>
</ul>
</li>
<li><p><strong>重复训练：</strong></p>
<ul>
<li><p><strong>迭代训练：</strong> 对整个训练集进行多次迭代（Epochs），不断优化专家和门控网络的参数，直到误差收敛或达到预设的训练轮数。<br><strong>具体细节：</strong></p>
</li>
<li><p><strong>专家网络的调整：</strong></p>
<ul>
<li>在原始误差函数下，所有专家的输出都会影响系统输出，因此所有专家都会根据各自的误差调整参数。</li>
<li>在变体误差函数下，梯度更倾向于那些表现较好的专家，促使这些专家更快地优化其输出。</li>
</ul>
</li>
<li><p><strong>门控网络的调整：</strong></p>
<ul>
<li>门控网络通过调整 $p_i^c$ 来优化专家的选择，使得系统输出更接近期望输出。</li>
<li>在变体误差函数下，门控网络会逐渐学会更倾向于选择那些能够更好地处理特定输入的专家。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="问题4：门控网络如何决定调用哪个专家网络？"><a href="#问题4：门控网络如何决定调用哪个专家网络？" class="headerlink" title="问题4：门控网络如何决定调用哪个专家网络？"></a><strong>问题4：门控网络如何决定调用哪个专家网络？</strong></h3><p><strong>理解门控网络的工作机制</strong><br><strong>门控网络的角色：</strong><br>门控网络的主要任务是根据输入数据决定使用哪个专家网络来处理当前样本。它充当“调度员”的角色，动态分配任务给最合适的专家。<br><strong>具体机制：</strong></p>
<ol>
<li><p><strong>输入传递：</strong></p>
<ul>
<li>门控网络接收与专家网络相同的输入向量 $\mathbf{x}^c$。</li>
</ul>
</li>
<li><p><strong>生成概率分布：</strong></p>
<ul>
<li>门控网络输出一组分数 $x_j$（未归一化的权重），每个分数对应一个专家网络。</li>
<li><p>这些分数通过 Softmax 函数转换为概率 $p_j$：</p>
<script type="math/tex; mode=display">
p_j = \frac{\exp(x_j)}{\sum_{i} \exp(x_i)}</script><ul>
<li><strong>Softmax 函数的作用：</strong> 将分数转化为一个概率分布，确保所有 $p_j$ 之和为 1。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>选择专家：</strong></p>
<ul>
<li><p><strong>确定性选择（Hard Selection）：</strong> 选择具有最高概率 $p_j$ 的专家 $j$ 作为当前样本的处理者。</p>
<ul>
<li><strong>优点：</strong> 简单高效，每次仅使用一个专家，降低计算开销。</li>
<li><strong>缺点：</strong> 可能导致门控网络过于偏向某些专家，忽略其他专家的潜力。</li>
</ul>
</li>
<li><p><strong>概率性选择（Soft Selection）：</strong> 根据概率分布 $p_j$ 随机选择一个专家 $j$。</p>
<ul>
<li><strong>优点：</strong> 保留多个专家的参与机会，促进更全面的专家训练。</li>
<li><strong>缺点：</strong> 可能导致训练不稳定，因为不同专家可能随机被选择。</li>
</ul>
</li>
<li><p><strong>混合策略：</strong> 在训练早期使用概率性选择，逐渐转向确定性选择，以平衡探索和利用。</p>
</li>
</ul>
</li>
<li><strong>训练期间的专家分配：</strong><ul>
<li>在训练过程中，门控网络通过误差函数的梯度反馈不断优化其决策，使得更适合处理特定输入的专家获得更高的选择概率。</li>
<li>特别是在使用变体误差函数时，门控网络更倾向于选择那些表现更好的专家，从而促进专家的专精化。</li>
</ul>
</li>
<li><strong>推理阶段的专家选择：</strong><ul>
<li><strong>确定性选择通常用于推理阶段，</strong> 以确保高效的计算和稳定的输出。</li>
<li>通过选择概率最高的专家，系统能够快速响应并生成准确的输出。</li>
</ul>
</li>
</ol>
<p><strong>图示说明：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">输入向量 x^c</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">+---------------+</span><br><span class="line">| 门控网络 G     |</span><br><span class="line">+---------------+</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">p_1, p_2, ..., p_n  (Softmax 概率分布)</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">选择专家 j（确定性或概率性）</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">+---------------+</span><br><span class="line">| 专家网络 E_j   |</span><br><span class="line">+---------------+</span><br><span class="line">      |</span><br><span class="line">      v</span><br><span class="line">系统输出 O^c = o_j^c</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Relativity suis</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">http://example.com/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Relativity suis's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MoE/">MoE</a></div><div class="post-share"><div class="social-share" data-image="/img/486.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</div></div><div class="info-2"><div class="info-item-1">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https://arxiv.org/abs/1701.06538 参考链接：https://zhuanlan.zhihu.com/p/542465517 Abstract The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increas-ing model capacity without a proportional increase in computation. In...</div></div></div></a><a class="pagination-related" href="/2025/01/06/RoPE/" title="RoPE"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">RoPE</div></div><div class="info-2"><div class="info-item-1">Rotary Position Embedding（RoPE） 是一种用于Transformer模型的位置信息编码方法，其核心思想是通过旋转操作将位置信息嵌入到查询（Query）和键（Key）向量中。这种方法不仅保留了相对位置信息的表达能力，还能与自注意力机制无缝集成，提升模型处理长序列的能力。本文将详细介绍RoPE的旋转机制，结合数学公式深入解析其工作原理。 1. 背景：位置编码在Transformer中的作用Transformer模型依赖自注意力机制来捕捉序列中元素之间的依赖关系。然而，自注意力机制本身不具备处理序列顺序的能力，因此需要通过位置编码来向模型提供位置信息。传统的位置编码方法，如绝对位置编码和相对位置编码，分别通过添加或修改嵌入向量来引入位置信息。RoPE则通过旋转操作，将位置信息直接嵌入到查询和键向量的几何结构中。 2. Rotary Position...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-07</div><div class="info-item-2">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</div></div><div class="info-2"><div class="info-item-1">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读论文链接：https://arxiv.org/abs/1701.06538 参考链接：https://zhuanlan.zhihu.com/p/542465517 Abstract The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increas-ing model capacity without a proportional increase in computation. In...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/486.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Relativity suis</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Gcy-shili"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Blog 积极更新中！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB"><span class="toc-number">1.</span> <span class="toc-text">Adaptive Mixtures of Local Experts 论文研读</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Making-Associative-Learning-Competitive"><span class="toc-number">1.2.</span> <span class="toc-text">Making Associative Learning Competitive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Making-Competitive-Learning-Associative"><span class="toc-number">1.3.</span> <span class="toc-text">Making Competitive Learning Associative</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3"><span class="toc-number">1.4.</span> <span class="toc-text">一些理解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="toc-number">1.4.1.</span> <span class="toc-text">一、背景与动机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.2.</span> <span class="toc-text">二、混合专家模型的基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E6%94%B9%E8%BF%9B%E6%96%B9%E5%90%91"><span class="toc-number">1.4.3.</span> <span class="toc-text">三、传统方法的问题与改进方向</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0%E4%B8%8E%E4%B8%93%E5%AE%B6%E7%BB%84%E5%90%88"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">传统的误差函数与专家组合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E6%96%B9%E5%90%91%EF%BC%9A%E9%BC%93%E5%8A%B1%E7%AB%9E%E4%BA%89%E8%80%8C%E9%9D%9E%E5%90%88%E4%BD%9C"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">改进方向：鼓励竞争而非合作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%AC%AC%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF-%E2%80%94%E2%80%94-%E4%BD%BF%E5%85%B3%E8%81%94%E5%AD%A6%E4%B9%A0%E5%85%B7%E6%9C%89%E7%AB%9E%E4%BA%89%E6%80%A7"><span class="toc-number">1.4.4.</span> <span class="toc-text">四、第一种实现思路 —— 使关联学习具有竞争性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%AC%AC%E4%BA%8C%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF-%E2%80%94%E2%80%94-%E4%BD%BF%E7%AB%9E%E4%BA%89%E5%AD%A6%E4%B9%A0%E5%85%B7%E6%9C%89%E8%81%94%E6%83%B3%E6%80%A7"><span class="toc-number">1.4.5.</span> <span class="toc-text">五、第二种实现思路 —— 使竞争学习具有联想性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%96%91%E9%97%AE"><span class="toc-number">1.5.</span> <span class="toc-text">一些疑问</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%981%EF%BC%9A%E5%9C%A8%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E4%B8%8A%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BD%93%E4%B8%80%E4%B8%AA%E4%B8%93%E5%AE%B6%E8%B0%83%E6%95%B4%E6%9D%83%E9%87%8D%E6%97%B6%EF%BC%8C%E4%BC%9A%E5%BD%B1%E5%93%8D%E5%89%A9%E4%BD%99%E4%B8%93%E5%AE%B6%E9%9C%80%E8%A6%81%E8%A1%A5%E5%81%BF%E7%9A%84%E6%AE%8B%E5%B7%AE%EF%BC%8C%E8%BF%99%E9%87%8C%E2%80%9C%E9%9C%80%E8%A6%81%E8%A1%A5%E5%81%BF%E7%9A%84%E6%AE%8B%E5%B7%AE%E2%80%9D%E7%9A%84%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D%EF%BC%9F"><span class="toc-number">1.5.1.</span> <span class="toc-text">问题1：在传统方法上，为什么当一个专家调整权重时，会影响剩余专家需要补偿的残差，这里“需要补偿的残差”的什么意思？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%982%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%9A%E8%BF%87%E5%AF%B9%E6%96%B0%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0%E4%B8%8E%E5%85%B6%E5%8F%98%E4%BD%93%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%BF%9B%E8%A1%8C%E6%AF%94%E8%BE%83%EF%BC%8C%E5%B0%B1%E8%83%BD%E5%BE%97%E5%87%BA%E5%85%B6%E5%8F%98%E4%BD%93%E6%95%88%E6%9E%9C%E6%9B%B4%E5%A5%BD%EF%BC%8C%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%AF%94%E8%BE%83%E7%9A%84%EF%BC%9F"><span class="toc-number">1.5.2.</span> <span class="toc-text">问题2：为什么通过对新误差函数与其变体的误差函数的导数进行比较，就能得出其变体效果更好，如何进行比较的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%983%EF%BC%9A%E8%BF%99%E7%A7%8D%E7%BD%91%E7%BB%9C%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%EF%BC%9F"><span class="toc-number">1.5.3.</span> <span class="toc-text">问题3：这种网络是如何进行训练的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%984%EF%BC%9A%E9%97%A8%E6%8E%A7%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%86%B3%E5%AE%9A%E8%B0%83%E7%94%A8%E5%93%AA%E4%B8%AA%E4%B8%93%E5%AE%B6%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-number">1.5.4.</span> <span class="toc-text">问题4：门控网络如何决定调用哪个专家网络？</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</a><time datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Adaptive-Mixtures-of-Local-Experts-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Adaptive Mixtures of Local Experts 论文研读">Adaptive Mixtures of Local Experts 论文研读</a><time datetime="2025-01-06T16:29:09.000Z" title="发表于 2025-01-07 00:29:09">2025-01-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/RoPE/" title="RoPE">RoPE</a><time datetime="2025-01-05T16:16:07.000Z" title="发表于 2025-01-06 00:16:07">2025-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/05/RMSNorm/" title="RMSNorm">RMSNorm</a><time datetime="2025-01-05T15:58:03.000Z" title="发表于 2025-01-05 23:58:03">2025-01-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/04/Attention%20Pt.1.%20Understanding-from-seq2seq-to-attention/" title="Understanding from seq2seq to attention">Understanding from seq2seq to attention</a><time datetime="2025-01-04T04:54:59.000Z" title="发表于 2025-01-04 12:54:59">2025-01-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Relativity suis</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">"Suis is all you need"</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>