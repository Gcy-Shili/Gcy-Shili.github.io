<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>emnlp2024论文研读-参数高效稀疏化 | Relativity suis's Blog</title><meta name="author" content="Relativity suis"><meta name="copyright" content="Relativity suis"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="EMNLP2024 论文研读 - 参数高效稀疏化论文：Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks 摘要 Large language models (LLMs) have demon-strated considerabl">
<meta property="og:type" content="article">
<meta property="og:title" content="emnlp2024论文研读-参数高效稀疏化">
<meta property="og:url" content="http://example.com/2025/01/14/emnlp2024%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96/index.html">
<meta property="og:site_name" content="Relativity suis&#39;s Blog">
<meta property="og:description" content="EMNLP2024 论文研读 - 参数高效稀疏化论文：Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks 摘要 Large language models (LLMs) have demon-strated considerabl">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/486.jpg">
<meta property="article:published_time" content="2025-01-13T18:39:15.000Z">
<meta property="article:modified_time" content="2025-01-17T14:14:30.874Z">
<meta property="article:author" content="Relativity suis">
<meta property="article:tag" content="emnlp2024">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/486.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/01/14/emnlp2024%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'emnlp2024论文研读-参数高效稀疏化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/486.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/suis1.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Relativity suis's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">emnlp2024论文研读-参数高效稀疏化</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">emnlp2024论文研读-参数高效稀疏化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-13T18:39:15.000Z" title="发表于 2025-01-14 02:39:15">2025-01-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-17T14:14:30.874Z" title="更新于 2025-01-17 22:14:30">2025-01-17</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="EMNLP2024-论文研读-参数高效稀疏化"><a href="#EMNLP2024-论文研读-参数高效稀疏化" class="headerlink" title="EMNLP2024 论文研读 - 参数高效稀疏化"></a>EMNLP2024 论文研读 - 参数高效稀疏化</h1><p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.02731">Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote>
<p>Large language models (LLMs) have demon-strated considerable proficiency in general natural language processing(NLP) tasks. Instruc-tion tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the in-struction tuning phase poses significant challenges. To address this issue, we introduce <strong>parameter-efficient sparsity crafting </strong>(PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architec-ture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory require-ments, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse up-cycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wuhy68/Parameter-Efficient-MoE">https://github.com/wuhy68/Parameter-Efficient-MoE</a>.</p>
</blockquote>
<p>这篇论文主要介绍了一种名为 PESC 的新方法，用于解决大型语言模型在指令微调过程中的容量限制问题。该方法通过 MoE 架构将密集模型转化为稀疏模型，并创新性地使用适配器（Adapters）来区分专家，而无需改变这些层的内部权重。这种方法不仅降低了计算和内存开销，还能在最小化参数增加的情况下有效扩展模型容量。实验结果表明，使用 PESC 方法训练的稀疏模型在性能上超过了其他模型，包括 GPT-3.5，证明了该方法的实用价值和效果。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>作者指出，训练 LLM 的一个显著方法是<strong>指令调优</strong>（Instruction Tuning），这种方式通过使用大规模、格式良好的指令数据训练 LLM，使 LLM 能够优化其预训练表示以符合人类指令，然而，这些任务固有的复杂性可能会阻碍模型微调，具体来说，某些规模的模型可能难以从冲突的任务中优化损失，导致通用任务的表现不佳。</p>
<p>The Scaling Law 表明增加模型的规模对提高模型表现至关重要，扩大模型的容量也可以提高对通用任务指令微调的有效性，然而，大多数 LLM 都是基于 Transformer 架构设计的预训练密集模型（dense model），这限制了指令微调过程中的可扩展性。Komatsuzaki et al.(2023) 提出了一种将密集模型改造为稀疏激活的 MoE 模型 的方法，并使模型具有了更大的容量；Shen et al.(2023) 指出与密集模型相比，MoE 模型对指令微调的响应更加有效，因此，在指令微调时将密集模型转换成 MoE 模型有可能在一般任务上取得优异表现。但是鉴于当前 LLM 的参数规模，训练这样的巨型模型需要更新 MoE 层中专家的权重，这受到 GPU 内存资源和计算成本的制约。</p>
<p>从以上描述可知，作者主要关注的问题是：</p>
<ol>
<li><p>将密集模型拓展到稀疏 MoE 模型以增大模型容量（增大模型容量可能带来提升效果；拓展为 MoE 模型可能对指令微调的响应更佳）</p>
</li>
<li><p>对 MoE 模型进行指令微调时，更新各专家的权重，会占用大量计算与内存资源（如何高效微调）</p>
</li>
</ol>
<p>作者提出了<strong>参数高效稀疏化构建</strong>（PESC）的方法，在有效拓展模型容量的同时能与 PEFT 协同工作。对于第一个问题，其实先前也有类似的解决方案，见论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.05055">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a>，而该篇论文在稀疏化构建的方法上很相似，我们可以对比两者的结构示意图：</p>
<p>Sparse Upcycling：</p>
<p><img src="/images/sparse_upcycling.png" alt="sparse_upcycling"></p>
<p>本文：</p>
<p><img src="/images/pesc.png" alt="pesc"></p>
<p>其主要的改进部分是在稀疏化后的 MoE 层中，在 FFN 的上面添加了 Adapters 适配层以利用 PEFT 的思路进行稀疏化后的训练，后面将详细分析。</p>
<h2 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h2><h3 id="Adapters"><a href="#Adapters" class="headerlink" title="Adapters"></a>Adapters</h3><p>首先介绍 Houlsby et al.(2019) 提出的一种将适配器集成到 Transformer-based 的预训练模型中的参数高效微调方法，这种方法只需要调整添加的 adapter 层的参数即可，一个适配器包括两个矩阵： $W<em>{down} \in \mathbb{R}^{d_1 \times d_2}$ 和 $W</em>{up} \in \mathbb{R}^{d_2 \times d_1}$，再加上一个非线性函数（激活函数），其中 $d_1$ 和 $d_2$ 分别表示预训练模型的特征维度（hidden_size）和适配器的隐藏维度（adapter hidden size），一般来说 $d_2 &lt; d_1$，给出预训练模型的特征 $U \in \mathbb{R}^{N \times d_1}$，适配器模块的输出为：</p>
<script type="math/tex; mode=display">
U' = \sigma\left( UW_{dowm} \right)W_{up} + U</script><p>我们来看一下在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.00751"> Adapter 原论文</a>中的介绍的高效微调方法，从图片中我们也可以直观的理解其计算过程：</p>
<p><img src="/images/adapter.png" alt="adapter"></p>
<p>在该论文中使用的 Adapter 与上图中的方法基本是一样的，该论文中具体的 MoE 层设计如下图所示：</p>
<p><img src="/images/ada-pesc.png" alt="ada-pesc"></p>
<h3 id="Mixture-of-Experts"><a href="#Mixture-of-Experts" class="headerlink" title="Mixture-of-Experts"></a>Mixture-of-Experts</h3><p>一个经典的专家混合的输出设计为（具体可参见论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a>，相关研读<a target="_blank" rel="noopener" href="https://gcy-shili.github.io/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-论文研读/">The Sparsely-Gated Mixture-of-Experts Layer 论文研读 | Relativity suis’s Blog</a>）：</p>
<script type="math/tex; mode=display">
y = \sum_{i=1}^n R(x)_iE_i(x) \tag{1}</script><p>其中 $R(x)$ 是门控网络的输出（筛选出真正要使用的专家，首先经过 $\texttt{KeepTopK}$ 筛选出前 $K$ 个专家，然后使用 $\texttt{softmax}$ 归一化生成权重），$E_i(x)$ 是第 $i$ 个专家的输出。</p>
<h3 id="Sparsity-Crafting"><a href="#Sparsity-Crafting" class="headerlink" title="Sparsity Crafting"></a>Sparsity Crafting</h3><p>基于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.05055">Sparse Upcycling</a> 的工作，其核心是利用原密集模型的权重，并涉及到一个变革性的过程：在原密集 Transformer 模型的每个 block 中，用 MoE 层替换 FFN 层，在稀疏性构建的初始化阶段，使用原密集模型的 FFN 的权重作为 MoE 层中每个专家的 FFN 模块的初始化权重，Adapter 层的权重为随机初始化，同时，为了确保结构的一致性，模型中的其他模块（如 Attention 层和 Norm 层等）直接从原模型中 copy 过来，现在再看模型的结构图我们也可以更好地理解。</p>
<p><img src="/images/ada-pesc.png" alt="ada-pesc"></p>
<h2 id="参数高效的稀疏性构建"><a href="#参数高效的稀疏性构建" class="headerlink" title="参数高效的稀疏性构建"></a>参数高效的稀疏性构建</h2><p>我们再仔细来看 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.05055">Sparse Upcycling</a> 中的稀疏性构建与训练过程，主要关注 MoE 层，在这篇工作里的作者将 MoE 层的所有专家设计为 MLP，初始化为对应 block 的 FFN 层的参数，因此在后面的训练过程中需要更新的参数就是所有块的所有专家（即 MLP）的所有参数，这其实就会造成大量的参数更新，从而需要很多的计算与存储资源，并导致训练时间变长（并不高效）。</p>
<p>而本文中作者来改善 / 缓解这一问题的方法就是，在专家的 FFN / MLP（本文中作者称为 FFN。其实都差不多）上添加 Adapters 适配器层，从而只需要通过<strong>更新适配器层的少量参数</strong>即可达成训练的目的（效果与前面的方法相比在可接受范围内，也就类似于全量微调与参数高效微调的关系），实际上也就是利用了 PEFT 的思想与方法，后面作者通过一些数学解释与文献引用，也说明了使用适配器的方法能够有效地保证近似质量（与专家参数全调整相比）。</p>
<p>我们可以再来对比两者的结构示意图，主要区别就在于专家层里：</p>
<p>Sparse Upcycling：</p>
<p><img src="/images/sparse_upcycling.png" alt="sparse_upcycling"></p>
<p>本文：</p>
<p><img src="/images/pesc.png" alt="pesc"></p>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><p>经过上述分析，我们可以更新方程 1 的表示：</p>
<script type="math/tex; mode=display">
y = \sum_{i=1}^n R(x)_iA_i(E(x))</script><script type="math/tex; mode=display">
A_i(x) = \sigma(xW_{i_{down}})W_{i_{up}} + x</script><p>这样我们更新的参数就不是整个 $E<em>i(x)$ ，而是适配器的参数 $W</em>{i<em>{down}},~W</em>{i_{up}}$</p>
<p>门控网络的设计没有什么变化：</p>
<script type="math/tex; mode=display">
R(x) = \texttt{softmax}(\texttt{KeepTopK}(W_{route} \cdot x))</script><p>对于负载均衡，论文中描述：</p>
<blockquote>
<p>Top-K 门控路由器通过其门控机制，往往会不成比例地偏 向某几个专家，导致这些专家更频繁地被训练和被路由器选择，为了解决这种不平衡并促进专家的均匀使用，我们在每个稀疏 Transformer 块的训练过程中引入了一个辅助损失函数（由 Fedus 等人(2022)提出）。对于 $n$ 个专家和包含 $T$ 个 token 的批次 $B$，专家负载均衡的辅助损失 $\mathcal{L}$ 计算为向量 $f$ 和 $p$ 的缩放点积：<br>数学公式：</p>
<script type="math/tex; mode=display">
\mathcal{L} = \alpha \cdot n \cdot \sum_{i=1}^n f_i \cdot p_i</script><p>其中 $f_i$ 表示分配给专家 $i$ 的 token 比例，$p_i$ 表示分配给专家i的路由概率比例。$\alpha$ 是辅助损失的乘性系数，我们使用 $\alpha = 10^{-2}$，这个值足够大以确保负载均衡，同时又足够小以不会压倒主要的交叉熵目标。理想情况下，应该在 $n$ 个专家之间实现均匀路由，因此两个向量的理想值都应该是 $\frac{1}{n}$，上述方程中的辅助损失促进了这种均匀分布，并在这种条件下达到最小值。</p>
</blockquote>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>在该篇工作中，模型实现的重点就是对模型从密集变换成稀疏模型的部分，与添加 Adapters 层的部分，在本文的<a target="_blank" rel="noopener" href="https://github.com/wuhy68/Parameter-Efficient-MoE/tree/master">代码仓库</a>中主要在 <code>./camelidae/modeling_camelidae.py</code> 中实现，我们看其中的一部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ParallelAdapterMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, adapter_dim, adapter_scaling</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.config = config</span><br><span class="line">        <span class="variable language_">self</span>.intermediate_size = config.intermediate_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = config.hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.adapter_down = nn.Linear(<span class="variable language_">self</span>.hidden_size, adapter_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.adapter_up = nn.Linear(adapter_dim, <span class="variable language_">self</span>.hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.adapter_act = nn.GELU()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.adapter_dropout = nn.Dropout(p=<span class="number">0.1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.adapter_scaling = adapter_scaling</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.adapter_dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.adapter_scaling * <span class="variable language_">self</span>.adapter_up(<span class="variable language_">self</span>.adapter_act(<span class="variable language_">self</span>.adapter_down(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CamelidaeGateAdapter</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config: CamelidaeConfig</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.intermediate_size = config.intermediate_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = config.hidden_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Router</span></span><br><span class="line">        <span class="variable language_">self</span>.num_experts = config.num_experts</span><br><span class="line">        <span class="variable language_">self</span>.topk = config.topk</span><br><span class="line">        <span class="variable language_">self</span>.router = nn.Linear(</span><br><span class="line">            config.hidden_size, <span class="variable language_">self</span>.num_experts, bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.dtype = <span class="built_in">getattr</span>(torch, config.moe_dtype)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: Get the experts</span></span><br><span class="line">        <span class="variable language_">self</span>.experts = nn.ModuleDict()</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(config.num_experts):</span><br><span class="line">            <span class="variable language_">self</span>.experts[<span class="string">f&quot;expert_<span class="subst">&#123;idx&#125;</span>&quot;</span>] = ParallelAdapterMLP(config, config.adapter_dim, config.moe_scaling)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_hidden_states, output_hidden_states, router_hidden_states</span>):</span><br><span class="line">        orig_shape = output_hidden_states.shape</span><br><span class="line">        input_hidden_states = input_hidden_states.view(-<span class="number">1</span>, input_hidden_states.shape[-<span class="number">1</span>])</span><br><span class="line">        output_hidden_states = output_hidden_states.view(-<span class="number">1</span>, output_hidden_states.shape[-<span class="number">1</span>])</span><br><span class="line">        router_hidden_states = router_hidden_states.view(-<span class="number">1</span>, router_hidden_states.shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        router_logits = <span class="variable language_">self</span>.router(router_hidden_states)</span><br><span class="line"></span><br><span class="line">        expert_weights, expert_indices = torch.topk(router_logits, <span class="variable language_">self</span>.topk, dim=-<span class="number">1</span>)</span><br><span class="line">        expert_weights = expert_weights.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        flat_expert_indices = expert_indices.view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        input_hidden_states = input_hidden_states.repeat_interleave(<span class="variable language_">self</span>.topk, dim=<span class="number">0</span>)</span><br><span class="line">        expert_hidden_states = output_hidden_states.repeat_interleave(<span class="variable language_">self</span>.topk, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> idx, expert <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.experts.values()):</span><br><span class="line">            expert_hidden_states[flat_expert_indices == idx] += expert(input_hidden_states[flat_expert_indices == idx])</span><br><span class="line">        hidden_states = (expert_hidden_states.view(*expert_weights.shape, -<span class="number">1</span>) * expert_weights.unsqueeze(-<span class="number">1</span>)).<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_states.view(*orig_shape), router_logits</span><br></pre></td></tr></table></figure>
<p><code>ParallelAdapterMLP</code> 类构建了添加到专家 FFN 后的适配器层，可以看到其 <code>forward</code> 方法与前面叙述的一致，然后在 <code>CamelidaeGateAdapter</code> 类中调用了该类，为每个专家添加一个适配器，在<code>CamelidaeGateAdapter</code> 类的 <code>forward</code> 中也可以看到 MoE 层中从输入门控网络得到输出分布，后经过 <code>KeepTopK</code> 和 <code>softmax</code> 操作得到门控的输出 $R(x)$，与处理专家的输入输出的过程。</p>
<h3 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h3><p>另外作者提到，在本文的研究中，对于 MoE 层的专家通过添加 Adapters 层进行微调，然后使用 QLoRA 对其他层进行微调，在 <code>train_moe.py</code> 中我们关注 <code>train</code> 函数，可以看到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    parser = transformers.HfArgumentParser(</span><br><span class="line">        (ModelArguments, DataArguments, TrainingArguments)</span><br><span class="line">    )</span><br><span class="line">    model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br><span class="line">    training_args.ddp_find_unused_parameters = <span class="literal">False</span></span><br><span class="line">    set_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    model_config = CamelidaeConfig.from_pretrained(model_args.model_name_or_path)</span><br><span class="line">    model_config.pretraining_tp = <span class="number">1</span>  <span class="comment">## without tensor parallelism rank</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Camelidae Config</span></span><br><span class="line">    model_config.moe_dtype = <span class="string">&quot;bfloat16&quot;</span></span><br><span class="line">    model_config.lora_r = <span class="number">64</span></span><br><span class="line">    model_config.lora_alpha = <span class="number">16</span></span><br><span class="line">    model_config.adapter_dim = <span class="number">64</span></span><br><span class="line">    model_config.topk = <span class="number">2</span></span><br><span class="line">    model_config.moe_scaling = <span class="number">1</span></span><br><span class="line">    model_config.num_experts = <span class="number">8</span></span><br><span class="line">    model_config.output_router_logits = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # Seq Length Extension</span></span><br><span class="line">    <span class="comment"># model_config.rope_scaling = &#123;</span></span><br><span class="line">    <span class="comment">#     &quot;type&quot;: &quot;dynamic&quot;,</span></span><br><span class="line">    <span class="comment">#     &quot;factor&quot;: 2,</span></span><br><span class="line">    <span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line">    model = LlamaForCausalLM.from_pretrained(</span><br><span class="line">        model_args.model_name_or_path,</span><br><span class="line">        config=model_config,</span><br><span class="line">        cache_dir=training_args.cache_dir,</span><br><span class="line">        load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">        quantization_config=BitsAndBytesConfig(</span><br><span class="line">            load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">            bnb_4bit_compute_dtype=torch.bfloat16,</span><br><span class="line">            bnb_4bit_use_double_quant=<span class="literal">True</span>,</span><br><span class="line">            bnb_4bit_quant_type=<span class="string">&quot;nf4&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        output_loading_info=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=<span class="literal">True</span>)</span><br><span class="line">    model.gradient_checkpointing_enable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># lora_modules = find_all_linear_names(model)</span></span><br><span class="line">    lora_modules = [</span><br><span class="line">        <span class="string">&quot;q_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;k_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;v_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;up_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;gate_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;down_proj&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line">    config = LoraConfig(</span><br><span class="line">        r=model_config.lora_r,</span><br><span class="line">        lora_alpha=model_config.lora_alpha,</span><br><span class="line">        target_modules=lora_modules,</span><br><span class="line">        lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">        bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">        task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    model = get_peft_model(model, config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero Init</span></span><br><span class="line">    <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;adapter_up&quot;</span> <span class="keyword">in</span> n:</span><br><span class="line">            nn.init.zeros_(p)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;adapter_down&quot;</span> <span class="keyword">in</span> n:</span><br><span class="line">            nn.init.kaiming_uniform_(p, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;router&quot;</span> <span class="keyword">in</span> n:</span><br><span class="line">            nn.init.kaiming_uniform_(p, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, LoraLayer):</span><br><span class="line">            <span class="keyword">if</span> training_args.bf16:</span><br><span class="line">                module = module.to(torch.bfloat16)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;norm&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">            module = module.to(torch.float32)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;lm_head&quot;</span> <span class="keyword">in</span> name <span class="keyword">or</span> <span class="string">&quot;embed_tokens&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(module, <span class="string">&quot;weight&quot;</span>):</span><br><span class="line">                <span class="keyword">if</span> training_args.bf16 <span class="keyword">and</span> module.weight.dtype == torch.float32:</span><br><span class="line">                    module = module.to(torch.bfloat16)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;adapter&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> training_args.bf16:</span><br><span class="line">                module = module.to(torch.bfloat16)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module = module.to(torch.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;adapter&quot;</span> <span class="keyword">in</span> n:</span><br><span class="line">            p.requires_grad = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># if &quot;norm&quot; in n:</span></span><br><span class="line">        <span class="comment">#     p.requires_grad = True</span></span><br><span class="line"></span><br><span class="line">    model.config.use_cache = <span class="literal">False</span></span><br><span class="line">    print_trainable_parameters(model)</span><br><span class="line"></span><br><span class="line">    tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">        model_args.model_name_or_path,</span><br><span class="line">        cache_dir=training_args.cache_dir,</span><br><span class="line">        model_max_length=training_args.model_max_length,</span><br><span class="line">        padding_side=<span class="string">&quot;right&quot;</span>,</span><br><span class="line">        use_fast=<span class="literal">False</span>,</span><br><span class="line">        trust_remote_code=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> tokenizer.pad_token <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        tokenizer.pad_token_id = (</span><br><span class="line">            <span class="number">0</span>  <span class="comment"># unk. we want this to be different from the eos token</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br><span class="line">    trainer = Trainer(</span><br><span class="line">        model=model, tokenizer=tokenizer, args=training_args, **data_module</span><br><span class="line">    )</span><br><span class="line">    trainer.add_callback(SavePeftModelCallback)</span><br><span class="line"></span><br><span class="line">    trainer.train()</span><br><span class="line"></span><br><span class="line">    model.save_pretrained(training_args.output_dir)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = LlamaForCausalLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    config=model_config,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">    quantization_config=BitsAndBytesConfig(</span><br><span class="line">        load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">        bnb_4bit_compute_dtype=torch.bfloat16,</span><br><span class="line">        bnb_4bit_use_double_quant=<span class="literal">True</span>,</span><br><span class="line">        bnb_4bit_quant_type=<span class="string">&quot;nf4&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    output_loading_info=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line">model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=<span class="literal">True</span>)</span><br><span class="line">model.gradient_checkpointing_enable()</span><br><span class="line"></span><br><span class="line"><span class="comment"># lora_modules = find_all_linear_names(model)</span></span><br><span class="line">lora_modules = [</span><br><span class="line">    <span class="string">&quot;q_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;k_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;v_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;up_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;gate_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;down_proj&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>是一些量化的配置与使用 LoRA 模块微调的模块，可以看到一共调整了注意力模块中的 <code>qkvo</code> 投影矩阵，门控矩阵与 MoE 层中的 FFN 模块，也就是说调整了除添加的 Adapters 的所有权重矩阵。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>作者还探讨了关于 Mixture of LoRA Experts 的相关内容，先在这里把翻译过来的部分贴出来：</p>
<blockquote>
<p>其他研究也探讨了将混合专家模型（MoE）与参数高效微调技术（PEFT）相结合的方法（Diao等，2023；Gou等，2023；Wu等，2024b；Liu等，2023；Luo等，2024；Dou等，2024）。例如，LoRAMoE（Dou等，2024）专注于世界知识的保留，而 MoELoRA（Luo等，2024）则利用统一了 MoE 和 LoRA 的 PEFT 框架，专注于数学和常识推理能力。然而，<strong>LoRA 框架的混合在训练和推理过程中带来了额外的计算成本，包括更高的内存占用和在没有并行化的情况下速度较慢</strong>。相比之下，我们的 PESC 方法则不会面临这些挑战。PESC 基于适配器模型框架，通过在复制的 FFN 层后插入多个适配器进行微调，而不是在相应的专家中微调所有复制的 FFN 层。在我们的 PESC 的 MoE 设计中，每个专家使用单一的适配器模块，与 LoRA 模块相比，显著减少了整体内存占用，因为 LoRA 模块由于其在 FFN 和注意力层中的位置，每个专家需要多个模块。这一区别在处理大量专家时尤为重要，因为内存限制变得越来越具有挑战性。此外，我们基于适配器的专家设计使得专家之间能够并行计算，因为它们彼此的输出相互独立，这与 LoRA 不同，LoRA 中层级之间的依赖关系可能会限制并行性。这种设计加速了训练时间，尤其是在专家数量增加的情况下，确保了可扩展性和效率。还值得注意的是，LoRA 在推理时可能需要将权重合并到主模型中，导致内存使用增加和潜在的延迟问题，特别是当多个令牌激活不同的专家时。相反，基于适配器的参数高效 MoE 在推理时不会产生这种开销，保持了与原始密集模型相似的低计算负担。</p>
</blockquote>
<p>这里其实没有太看明白作者是在拿自己的方法跟具体怎样使用 LoRA 的结果进行对比，不过作者前面提到了 LoRAMoE 和 MoELoRA 的工作，我们也先来看看：</p>
<p>LoRAMoE：</p>
<p><img src="/images/LoRAMoE.png" alt="LoRAMoE"></p>
<p>从示意图中可以很清楚地看出模型的工作原理，将原模型的其他模块迁移到 LoRAMoE 结构中，并保持参数冻结（包括 FFN 层），然后将 LoRA 模块视为 MoE 层的所有专家，通过门控网络控制专家的输出，再与 FFN 的输出相加。简单来说也就是冻结主干模型，引入多个 LoRA 适配器，使用路由网络（门控）整合这些适配器</p>
<p>MoELoRA：</p>
<p><img src="/images/MoELoRA.png" alt="MoELoRA"></p>
<p>MoELoRA 则是将 LoRA 视为一个专家系统，我们可以最后做一个对比：</p>
<ol>
<li>该篇工作（PESC）的设计与 LoRAMoE 的设计具有一些相似之处，其都保持了 Norm 和 Attention 等层的参数不变（冻结），而专注于处理 MoE 层的变化，PESC将 Transformer Block 中的 FFN 复制为 $N$ 份，作为 $N$ 个专家的 FFN 层的初始化，然后在这些 FFN 层后添加 Adapters 适配器层；LoRAMoE 则是将 Transformer Block 中的 FFN 保持冻结，同时使用若干 LoRA 模块作为专家使用，最后将专家（经门控）的输出与 FFN 的输出结合得到最终结果。</li>
<li>MoELoRA 则是将 LoRA 本身视为专家系统，将这一原本用于微调的 LoRA 模块改变为由 LoRA 模块组成的专家系统。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Relativity suis</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/01/14/emnlp2024%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96/">http://example.com/2025/01/14/emnlp2024%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Relativity suis's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/emnlp2024/">emnlp2024</a></div><div class="post-share"><div class="social-share" data-image="/img/486.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/01/09/%E5%AD%A6%E4%B9%A0-Transformer-%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%81%E5%8F%82%E6%95%B0%E5%8C%96%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96/" title="学习 Transformer 的初始化、参数化与标准化"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">学习 Transformer 的初始化、参数化与标准化</div></div><div class="info-2"><div class="info-item-1">尝试学习苏神的文章：https://kexue.fm/archives/8620 并做一些记录： 采样分布🤔模型的初始化是随机采样的，一般情况下我们都是从指定均值 $\mu$ 和方差 $\sigma^2$ 的随机分布中进行采样来初始化，其中常用的随机分布有三个：正态分布，均匀分布，截尾正态分布（Truncated Normal）。 其中正态分布通常记为 $\mathcal{N}(\mu, \sigma^2)$；区间 $[a,b]$ 上的均匀分布一般记为 $U[a,b]$，其均值为 $\frac{a+b}{2}$，方差为 $\frac{(b-a)^2}{12}$，所以如果指定 $u$ 和 $\sigma^2$ 的话，对应的均匀分布为 $U[\mu-\sqrt{3}\sigma,\mu+\sqrt{3}\sigma]$。 一般来说正态分布的采样结果更多样化一些，但理论上他是无界的，如果采样到绝对值过大的结果可能不利于优化；而均匀分布是有界的，但是采样结果更单一。结合两者优点即可得到 截尾正态分布，他从 $\mathcal{N}(\mu, \sigma^2)$...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/486.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Relativity suis</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Gcy-shili"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Blog 积极更新中！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#EMNLP2024-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text">EMNLP2024 论文研读 - 参数高效稀疏化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E8%AE%BA"><span class="toc-number">1.3.</span> <span class="toc-text">方法论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Adapters"><span class="toc-number">1.3.1.</span> <span class="toc-text">Adapters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mixture-of-Experts"><span class="toc-number">1.3.2.</span> <span class="toc-text">Mixture-of-Experts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sparsity-Crafting"><span class="toc-number">1.3.3.</span> <span class="toc-text">Sparsity Crafting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%9A%84%E7%A8%80%E7%96%8F%E6%80%A7%E6%9E%84%E5%BB%BA"><span class="toc-number">1.4.</span> <span class="toc-text">参数高效的稀疏性构建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.5.</span> <span class="toc-text">模型设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.6.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">1.6.1.</span> <span class="toc-text">模型构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#QLoRA"><span class="toc-number">1.6.2.</span> <span class="toc-text">QLoRA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">1.7.</span> <span class="toc-text">其他</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/14/emnlp2024%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB-%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E7%A8%80%E7%96%8F%E5%8C%96/" title="emnlp2024论文研读-参数高效稀疏化">emnlp2024论文研读-参数高效稀疏化</a><time datetime="2025-01-13T18:39:15.000Z" title="发表于 2025-01-14 02:39:15">2025-01-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/09/%E5%AD%A6%E4%B9%A0-Transformer-%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%81%E5%8F%82%E6%95%B0%E5%8C%96%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96/" title="学习 Transformer 的初始化、参数化与标准化">学习 Transformer 的初始化、参数化与标准化</a><time datetime="2025-01-09T12:50:27.000Z" title="发表于 2025-01-09 20:50:27">2025-01-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/08/LoRA-%E5%8F%8A%E5%85%B6%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="LoRA 及其论文研读">LoRA 及其论文研读</a><time datetime="2025-01-08T12:47:34.000Z" title="发表于 2025-01-08 20:47:34">2025-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/08/MoE-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="MoE 论文研读">MoE 论文研读</a><time datetime="2025-01-08T08:27:49.000Z" title="发表于 2025-01-08 16:27:49">2025-01-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/07/Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer 论文研读</a><time datetime="2025-01-07T09:40:30.000Z" title="发表于 2025-01-07 17:40:30">2025-01-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Relativity suis</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">"Suis is all you need"</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>